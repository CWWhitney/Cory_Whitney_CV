@article{2022-rich-screen-reader-vis-experiences,
  title = {Rich Screen Reader Experiences for Accessible Data Visualization},
  author = {Zong, Jonathan and Lee, Crystal and Lundgard, Alan and Jang, JiWoong and Hajas, Daniel and Satyanarayan, Arvind},
  date = {2022},
  journaltitle = {Computer Graphics Forum (Proc. EuroVis)},
  url = {http://vis.csail.mit.edu/pubs/rich-screen-reader-vis-experiences}
}

@online{AccessibleCOVID19Data,
  title = {Accessible {{COVID-19}} Data},
  url = {https://covid.ski.org/?fbclid=IwAR0kqAZIeQkyelOjMpRA_NrKVM8gKYGEVSZeFgT0vSe61f8aLE0z4oB8DzI},
  urldate = {2022-08-21},
  file = {C:\Users\jseo1005\Zotero\storage\D2ILW7QE\covid.ski.org.html}
}

@online{AccessibleGraphs,
  title = {Accessible {{Graphs}}},
  url = {https://accessiblegraphs.org/},
  urldate = {2023-09-11},
  abstract = {Helping blind people see graphs using sound and touch},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\DST4J3U2\accessiblegraphs.org.html}
}

@software{AccessibleGraphsProject2023,
  title = {The {{Accessible Graphs}} Project},
  date = {2023-01-17T09:12:00Z},
  origdate = {2019-10-10T21:52:42Z},
  url = {https://github.com/hasadna/accessible-graphs},
  urldate = {2023-09-11},
  abstract = {The Accessible Graphs project},
  organization = {{The Public Knowledge Workshop}}
}

@article{ACMDSeminarFully2017,
  title = {{{ACMD Seminar}}: {{Towards Fully Accessible Data Visualisation}}},
  shorttitle = {{{ACMD Seminar}}},
  date = {2017-06-06T09:43-04:00},
  journaltitle = {NIST},
  url = {https://www.nist.gov/itl/math/acmd-seminar-towards-fully-accessible-data-visualisation},
  urldate = {2022-08-21},
  abstract = {Volker SorgeSchool of Computer Science, University of Birmingham, UK},
  langid = {english},
  annotation = {Last Modified: 2019-11-15T19:42-05:00},
  file = {C:\Users\jseo1005\Zotero\storage\8ZZW8M5Y\acmd-seminar-towards-fully-accessible-data-visualisation.html}
}

@online{alamSeeChartEnablingAccessible2023,
  title = {{{SeeChart}}: {{Enabling Accessible Visualizations Through Interactive Natural Language Interface For People}} with {{Visual Impairments}}},
  shorttitle = {{{SeeChart}}},
  author = {Alam, Md Zubair Ibne and Islam, Shehnaz and Hoque, Enamul},
  date = {2023-02-15},
  eprint = {2302.07742},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.1145/3581641.3584099},
  url = {http://arxiv.org/abs/2302.07742},
  urldate = {2023-02-16},
  abstract = {Web-based data visualizations have become very popular for exploring data and communicating insights. Newspapers, journals, and reports regularly publish visualizations to tell compelling stories with data. Unfortunately, most visualizations are inaccessible to readers with visual impairments. For many charts on the web, there are no accompanying alternative (alt) texts, and even if such texts exist they do not adequately describe important insights from charts. To address the problem, we first interviewed 15 blind users to understand their challenges and requirements for reading data visualizations. Based on the insights from these interviews, we developed SeeChart, an interactive tool that automatically deconstructs charts from web pages and then converts them to accessible visualizations for blind people by enabling them to hear the chart summary as well as to interact through data points using the keyboard. Our evaluation with 14 blind participants suggests the efficacy of SeeChart in understanding key insights from charts and fulfilling their information needs while reducing their required time and cognitive burden.},
  pubstate = {preprint},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {C:\Users\jseo1005\Zotero\storage\6M9NTXPQ\Alam et al. - 2023 - SeeChart Enabling Accessible Visualizations Throu.pdf}
}

@online{americanprintinghousefortheblindAnnualReports2021,
  title = {Annual {{Reports}}},
  author = {{American Printing House for the Blind}},
  date = {2021},
  url = {https://www.aph.org/app/uploads/2022/04/annual-report-fy2021.pdf},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\UU2U8PQE\Limitless Possibility.pdf}
}

@online{AudioGraphsApple,
  title = {Audio {{Graphs}} | {{Apple Developer Documentation}}},
  url = {https://developer.apple.com/documentation/accessibility/audio_graphs},
  urldate = {2022-08-21},
  file = {C:\Users\jseo1005\Zotero\storage\3Y4533ZN\audio_graphs.html}
}

@inproceedings{aultEvaluationLongDescriptions2002,
  title = {Evaluation of {{Long Descriptions}} of {{Statistical Graphics}} for {{Blind}} and {{Low Vision Web Users}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author = {Ault, H. K. and Deloge, J. W. and Lapp, R. W. and Morgan, M. J. and Barnett, J. R.},
  editor = {Miesenberger, Klaus and Klaus, Joachim and Zagler, Wolfgang},
  date = {2002},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {517--526},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45491-8_99},
  abstract = {The objective of this research was to maximize not only accessibility but also user comprehension of web pages, particularly those containing tabular and graphical information. Based on literature and interviews with blind and low vision students and their teachers, the research team developed guidelines for web developers to describe charts and graphs commonly used in statistical applications. A usability study was then performed to evaluate the effectiveness of these new guidelines. Accessibility and comprehension for both blind and low vision users were increased when web pages were developed following the new guidelines.},
  isbn = {978-3-540-45491-5},
  langid = {english},
  keywords = {Accessibility Guideline,Blind User,Lesson Plan,Screen Reader,Worcester Polytechnic Institute},
  file = {C:\Users\jseo1005\Zotero\storage\ZPH9N8YJ\3-540-45491-8_99.pdf}
}

@inproceedings{banovicUncoveringInformationNeeds2013,
  title = {Uncovering Information Needs for Independent Spatial Learning for Users Who Are Visually Impaired},
  booktitle = {Proceedings of the 15th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Banovic, Nikola and Franz, Rachel L. and Truong, Khai N. and Mankoff, Jennifer and Dey, Anind K.},
  date = {2013-10-21},
  pages = {1--8},
  publisher = {{ACM}},
  location = {{Bellevue Washington}},
  doi = {10.1145/2513383.2513445},
  url = {https://dl.acm.org/doi/10.1145/2513383.2513445},
  urldate = {2022-08-21},
  eventtitle = {{{ASSETS}} '13: {{The}} 15th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  isbn = {978-1-4503-2405-2},
  langid = {english}
}

@article{bovairAcquisitionPerformanceTextEditing1990,
  title = {The {{Acquisition}} and {{Performance}} of {{Text-Editing Skill}}: {{A Cognitive Complexity Analysis}}},
  shorttitle = {The {{Acquisition}} and {{Performance}} of {{Text-Editing Skill}}},
  author = {Bovair, Susan and Kieras, David E. and Polson, Peter G.},
  date = {1990-03-01},
  journaltitle = {Human–Computer Interaction},
  volume = {5},
  number = {1},
  pages = {1--48},
  publisher = {{Taylor \& Francis}},
  issn = {0737-0024},
  doi = {10.1207/s15327051hci0501_1},
  url = {https://www.tandfonline.com/doi/abs/10.1207/s15327051hci0501_1},
  urldate = {2023-09-07},
  abstract = {Kieras and Polson (1985) proposed an approach for making quantitative predictions on ease of learning and ease of use of a system, based on a production system version of the goals, operators, methods, and selection rules (GOMS) model of Card, Moran, and Newel1 (1983). This article describes the principles for constructing such models and obtaining predictions of learning and execution time. A production rule model for a simulated text editor is described in detail and is compared to experimental data on learning and performance. The model accounted well for both learning and execution time and for the details of the increase in speed with practice. The relationship between the performance model and the Keystroke-Level Model of Card et al. (1983) is discussed. The results provide strong support for the original proposal that production rule models can make quantitative predictions for both ease of learning and ease of use.},
  file = {C:\Users\jseo1005\Zotero\storage\FUDR4HX3\Bovair et al. - 1990 - The Acquisition and Performance of Text-Editing Skill A Cognitive Complexity Analysis.pdf}
}

@online{BraillePatterns,
  title = {Braille {{Patterns}}},
  url = {https://unicode.org/charts/nameslist/c_2800.html},
  urldate = {2023-01-12},
  file = {C:\Users\jseo1005\Zotero\storage\EAIRJ2XZ\c_2800.html}
}

@article{brookeSUSRetrospective2013,
  title = {{{SUS}}: A Retrospective},
  shorttitle = {{{SUS}}},
  author = {Brooke, John},
  date = {2013-01-01},
  journaltitle = {Journal of Usability Studies},
  shortjournal = {Journal of Usability Studies},
  volume = {8},
  pages = {29--40}
}

@inproceedings{brownVizTouchAutomaticallyGenerated2012a,
  title = {{{VizTouch}}: Automatically Generated Tactile Visualizations of Coordinate Spaces},
  shorttitle = {{{VizTouch}}},
  booktitle = {Proceedings of the {{Sixth International Conference}} on {{Tangible}}, {{Embedded}} and {{Embodied Interaction}}},
  author = {Brown, Craig and Hurst, Amy},
  date = {2012-02-19},
  pages = {131--138},
  publisher = {{ACM}},
  location = {{Kingston Ontario Canada}},
  doi = {10.1145/2148131.2148160},
  url = {https://dl.acm.org/doi/10.1145/2148131.2148160},
  urldate = {2023-09-09},
  eventtitle = {{{TEI}}'12: {{Sixth International Conference}} on {{Tangible}}, {{Embedded}}, and {{Embodied Interaction}}},
  isbn = {978-1-4503-1174-8},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\WEMJHXZ2\Brown and Hurst - 2012 - VizTouch automatically generated tactile visualiz.pdf}
}

@online{ChartsContentComponents,
  title = {Charts - {{Content}} - {{Components}} - {{Human Interface Guidelines}} - {{Design}} - {{Apple Developer}}},
  url = {https://developer.apple.com/design/human-interface-guidelines/components/content/charts},
  urldate = {2022-09-27},
  file = {C:\Users\jseo1005\Zotero\storage\NQVXZWEW\charts.html}
}

@online{cherukuruVisualsExaminingExperiences2022,
  title = {Beyond {{Visuals}} : {{Examining}} the {{Experiences}} of {{Geoscience Professionals With Vision Disabilities}} in {{Accessing Data Visualizations}}},
  shorttitle = {Beyond {{Visuals}}},
  author = {Cherukuru, Nihanth W. and Bailey, David A. and Fourment, Tiffany and Hatheway, Becca and Holland, Marika M. and Rehme, Matt},
  date = {2022-07-26},
  eprint = {2207.13220},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.13220},
  url = {http://arxiv.org/abs/2207.13220},
  urldate = {2022-10-20},
  abstract = {Data visualizations are ubiquitous in all disciplines and have become the primary means of analysing data and communicating insights. However, the predominant reliance on visual encoding of data continues to create accessibility barriers for people who are blind/vision impaired resulting in their under representation in Science, Technology, Engineering and Mathematics (STEM) disciplines. This research study seeks to understand the experiences of professionals who are blind/vision impaired in one such STEM discipline (geosciences) in accessing data visualizations. In-depth, semi-structured interviews with seven professionals were conducted to examine the accessibility barriers and areas for improvement to inform accessibility research pertaining to data visualizations through a socio-technical lens. A reflexive thematic analysis revealed the negative impact of visualizations in influencing their career path, lack of data exploration tools for research, barriers in accessing works of peers and mismatched pace of visualization and accessibility research. The article also includes recommendations from the participants to address some of these accessibility barriers.},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society,Computer Science - Graphics,Computer Science - Human-Computer Interaction},
  file = {C:\Users\jseo1005\Zotero\storage\B2F2B2R2\Cherukuru et al. - 2022 - Beyond Visuals  Examining the Experiences of Geos.pdf}
}

@inproceedings{choiTactileDisplayBraille2004,
  title = {Tactile Display as a {{Braille}} Display for the Visually Disabled},
  booktitle = {2004 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}}) ({{IEEE Cat}}. {{No}}.{{04CH37566}})},
  author = {Choi, H.R. and Lee, S.W. and Jung, K.M. and Koo, J.C. and Lee, S.I. and Choi, H.G. and Jeon, J.W. and Nam, J.D.},
  date = {2004-09},
  volume = {2},
  pages = {1985-1990 vol.2},
  doi = {10.1109/IROS.2004.1389689},
  abstract = {Tactile sensation is one of the most important sensory functions along with the auditory sensation for the visually impaired because it replaces the visual sensation of the persons with sight. In this paper, we present a tactile display device as a dynamic Braille display that is the unique tool for exchanging information among them. The proposed tactile cell of the Braille display is based on the dielectric elastomer and it has advantageous features over the existing ones with respect to intrinsic softness, ease of fabrication, cost effectiveness and miniaturization. We introduce a new idea for actuation and describe the actuating mechanism of the Braille pin in details capable of realizing the enhanced spatial density of the tactile cells. Finally, results of psychophysical experiments are given and its effectiveness is confirmed.},
  eventtitle = {2004 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}}) ({{IEEE Cat}}. {{No}}.{{04CH37566}})},
  keywords = {Actuators,Auditory displays,Engineering management,Fabrication,Humans,Lungs,Mechanical engineering,Pins,Psychology,Skin},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\W28ZJ4ZI\\Choi et al. - 2004 - Tactile display as a Braille display for the visua.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\QT327JMP\\1389689.html}
}

@article{choiVisualizingNonVisual2019,
  title = {Visualizing for the {{Non}}‐{{Visual}}: {{Enabling}} the {{Visually Impaired}} to {{Use Visualization}}},
  shorttitle = {Visualizing for the {{Non}}‐{{Visual}}},
  author = {Choi, Jinho and Jung, Sanghun and Park, Deok Gun and Choo, Jaegul and Elmqvist, Niklas},
  date = {2019-06},
  journaltitle = {Computer Graphics Forum},
  volume = {38},
  number = {3},
  pages = {249--260},
  publisher = {{Wiley-Blackwell}},
  issn = {01677055},
  doi = {10.1111/cgf.13686},
  url = {https://proxy2.library.illinois.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bsu&AN=137771620&site=eds-live&scope=site},
  urldate = {2023-09-04},
  abstract = {The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep‐neural‐network‐based approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back‐end algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.},
  keywords = {CCS Concepts,Data modeling,Data visualization,Google Chrome (Computer software),Human‐centered computing → Visual analytics,People with visual disabilities,Visual analytics,Visualization,Visualization toolkits,Work design},
  file = {C:\Users\jseo1005\Zotero\storage\CR795AQV\Choi et al. - 2019 - Visualizing for the Non‐Visual Enabling the Visua.pdf}
}

@article{choiVisualizingNonVisualEnabling2019,
  title = {Visualizing for the {{Non-Visual}}: {{Enabling}} the {{Visually Impaired}} to {{Use Visualization}}},
  shorttitle = {Visualizing for the {{Non-Visual}}},
  author = {Choi, Jinho and Jung, Sanghun and Park, Deok Gun and Choo, Jaegul and Elmqvist, Niklas},
  date = {2019},
  journaltitle = {Computer Graphics Forum},
  volume = {38},
  number = {3},
  pages = {249--260},
  issn = {1467-8659},
  doi = {10.1111/cgf.13686},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13686},
  urldate = {2022-08-22},
  abstract = {The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep-neural-network-based approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back-end algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.},
  langid = {english},
  keywords = {• Human-centered computing → Visual analytics,CCS Concepts,Visualization toolkits},
  file = {C:\Users\jseo1005\Zotero\storage\ZBUN9DYE\cgf.html}
}

@inproceedings{ciuhaVisualizationConcurrentTones2010,
  title = {Visualization of Concurrent Tones in Music with Colours},
  booktitle = {Proceedings of the International Conference on {{Multimedia}} - {{MM}} '10},
  author = {Ciuha, Peter and Klemenc, Bojan and Solina, Franc},
  date = {2010},
  pages = {1677},
  publisher = {{ACM Press}},
  location = {{Firenze, Italy}},
  doi = {10.1145/1873951.1874320},
  url = {http://dl.acm.org/citation.cfm?doid=1873951.1874320},
  urldate = {2022-08-21},
  eventtitle = {The International Conference},
  isbn = {978-1-60558-933-6},
  langid = {english}
}

@article{clarkDualCodingTheory1991,
  title = {Dual Coding Theory and Education},
  author = {Clark, James M. and Paivio, Allan},
  date = {1991-09-01},
  journaltitle = {Educational Psychology Review},
  shortjournal = {Educ Psychol Rev},
  volume = {3},
  number = {3},
  pages = {149--210},
  issn = {1573-336X},
  doi = {10.1007/BF01320076},
  url = {https://doi.org/10.1007/BF01320076},
  urldate = {2023-01-17},
  abstract = {Dual coding theory (DCT) explains human behavior and experience in terms of dynamic associative processes that operate on a rich network of modality-specific verbal and nonverbal (or imagery) representations. We first describe the underlying premises of the theory and then show how the basic DCT mechanisms can be used to model diverse educational phenomena. The research demonstrates that concreteness, imagery, and verbal associative processes play major roles in various educational domains: the representation and comprehension of knowledge, learning and memory of school material, effective instruction, individual differences, achievement motivation and test anxiety, and the learning of motor skills. DCT also has important implications for the science and practice of educational psychology — specifically, for educational research and teacher education. We show not only that DCT provides a unified explanation for diverse topics in education, but also that its mechanistic framework accommodates theories cast in terms of strategies and other high-level psychological processes. Although much additional research needs to be done, the concrete models that DCT offers for the behavior and experience of students, teachers, and educational psychologists further our understanding of educational phenomena and strengthen related pedagogical practices.},
  langid = {english},
  keywords = {imagery,unified educational theory,verbal processes},
  file = {C:\Users\jseo1005\Zotero\storage\7WPYLRAX\Clark and Paivio - 1991 - Dual coding theory and education.pdf}
}

@inproceedings{degreefInterdependentVariablesRemotely2021,
  title = {Interdependent {{Variables}}: {{Remotely Designing Tactile Graphics}} for an {{Accessible Workflow}}},
  shorttitle = {Interdependent {{Variables}}},
  booktitle = {The 23rd {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {De Greef, Lilian and Moritz, Dominik and Bennett, Cynthia},
  date = {2021-10-17},
  pages = {1--6},
  publisher = {{ACM}},
  location = {{Virtual Event USA}},
  doi = {10.1145/3441852.3476468},
  url = {https://dl.acm.org/doi/10.1145/3441852.3476468},
  urldate = {2023-09-06},
  abstract = {In this experience report, we offer a case study of blind and sighted colleagues creating an accessible workflow to collaborate on a data visualization-focused project. We outline our process for making the project’s shared data representations accessible through incorporating both handmade and machine-embossed tactile graphics. We also share lessons and strategies for considering team needs and addressing contextual constraints like remote collaboration during the COVID-19 pandemic. More broadly, this report contributes to ongoing research into the ways accessibility is interdependent by arguing that access work must be a collective responsibility and properly supported with recognition, resources, and infrastructure.},
  eventtitle = {{{ASSETS}} '21: {{The}} 23rd {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  isbn = {978-1-4503-8306-6},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\YGPSINZ3\De Greef et al. - 2021 - Interdependent Variables Remotely Designing Tacti.pdf}
}

@online{DesmosGraphingCalculator,
  title = {Desmos | {{Graphing Calculator}}},
  url = {https://www.desmos.com/calculator},
  urldate = {2023-01-02},
  abstract = {Explore math with our beautiful, free online graphing calculator. Graph functions, plot points, visualize algebraic equations, add sliders, animate graphs, and more.},
  langid = {english},
  organization = {{Desmos}},
  file = {C:\Users\jseo1005\Zotero\storage\VTL3FFZK\calculator.html}
}

@online{Diagcess,
  title = {Diagcess},
  url = {https://www.npmjs.com/package/diagcess},
  urldate = {2022-08-21},
  abstract = {A diagram explorer for progressiveaccee.com style diagram annotations.. Latest version: 1.1.4, last published: 7 months ago. Start using diagcess in your project by running `npm i diagcess`. There are no other projects in the npm registry using diagcess.},
  langid = {english},
  organization = {{npm}},
  file = {C:\Users\jseo1005\Zotero\storage\NJM4U6JH\diagcess.html}
}

@article{dowWizardOzSupport2005,
  title = {Wizard of {{Oz}} Support throughout an Iterative Design Process},
  author = {Dow, S. and MacIntyre, B. and Lee, J. and Oezbek, C. and Bolter, J.D. and Gandy, M.},
  date = {2005-10},
  journaltitle = {IEEE Pervasive Computing},
  volume = {4},
  number = {4},
  pages = {18--26},
  issn = {1558-2590},
  doi = {10.1109/MPRV.2005.93},
  abstract = {The Wizard of Oz prototyping approach, widely used in human-computer interaction research, is particularly useful in exploring user interfaces for pervasive, ubiquitous, or mixed-reality systems that combine complex sensing and intelligent control logic. The vast design space for such nontraditional interfaces provides many possibilities for user interaction through one or more modalities and often requires challenging hardware and software implementations. The WOz method helps designers avoid getting locked into a particular design or working under an incorrect set of assumptions about user preferences, because it lets them explore and evaluate designs before investing the considerable development time needed to build a complete prototype.},
  eventtitle = {{{IEEE Pervasive Computing}}},
  keywords = {audio tours,Computational modeling,design process,HCI methods,Intelligent control,Intelligent sensors,Intelligent systems,Iterative methods,mixed reality,Process design,Prototypes,prototyping,Sensor systems,ubiquitous computing,User interfaces,Virtual reality,Wizard of Oz},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\FEII3JJW\\Dow et al. - 2005 - Wizard of Oz support throughout an iterative desig.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\AXPNQFXY\\1541964.html}
}

@inproceedings{ebelVisualizingEventSequence2021,
  title = {Visualizing {{Event Sequence Data}} for {{User Behavior Evaluation}} of {{In-Vehicle Information Systems}}},
  booktitle = {13th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Ebel, Patrick and Lingenfelder, Christoph and Vogelsang, Andreas},
  date = {2021-09-20},
  series = {{{AutomotiveUI}} '21},
  pages = {219--229},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3409118.3475140},
  url = {https://dl.acm.org/doi/10.1145/3409118.3475140},
  urldate = {2023-09-07},
  abstract = {With modern In-Vehicle Information Systems (IVISs) becoming more capable and complex than ever, their evaluation becomes increasingly difficult. The analysis of large amounts of user behavior data can help to cope with this complexity and can support UX experts in designing IVISs that serve customer needs and are safe to operate while driving. We, therefore, propose a Multi-level User Behavior Visualization Framework providing effective visualizations of user behavior data that is collected via telematics from production vehicles. Our approach visualizes user behavior data on three different levels: (1) The Task Level View aggregates event sequence data generated through touchscreen interactions to visualize user flows. (2) The Flow Level View allows comparing the individual flows based on a chosen metric. (3) The Sequence Level View provides detailed insights into touch interactions, glance, and driving behavior. Our case study proves that UX experts consider our approach a useful addition to their design process.},
  isbn = {978-1-4503-8063-8},
  file = {C:\Users\jseo1005\Zotero\storage\27AG4E2E\Ebel et al. - 2021 - Visualizing Event Sequence Data for User Behavior Evaluation of In-Vehicle Information Systems.pdf}
}

@article{elavskyHowAccessibleMy2022,
  title = {How Accessible Is My Visualization? {{Evaluating}} Visualization Accessibility with {{Chartability}}},
  shorttitle = {How Accessible Is My Visualization?},
  author = {Elavsky, Frank and Bennett, Cynthia and Moritz, Dominik},
  date = {2022-06},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {41},
  number = {3},
  pages = {57--70},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.14522},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14522},
  urldate = {2022-10-17},
  abstract = {Novices and experts have struggled to evaluate the accessibility of data visualizations because there are no common shared guidelines across environments, platforms, and contexts in which data visualizations are authored. Between non-specifc standards bodies like WCAG, emerging research, and guidelines from specifc communities of practice, it is hard to organize knowledge on how to evaluate accessible data visualizations. We present Chartability, a set of heuristics synthesized from these various sources which enables designers, developers, researchers, and auditors to evaluate data-driven visualizations and interfaces for visual, motor, vestibular, neurological, and cognitive accessibility. In this paper, we outline our process of making a set of heuristics and accessibility principles for Chartability and highlight key features in the auditing process. Working with participants on real projects, we found that data practitioners with a novice level of accessibility skills were more confdent and found auditing to be easier after using Chartability. Expert accessibility practitioners were eager to integrate Chartability into their own work. Refecting on Chartability’s development and the preliminary user evaluation, we discuss tradeoffs of open projects, working with high-risk evaluations like auditing projects in the wild, and challenge future research projects at the intersection of visualization and accessibility to consider the broad intersections of disabilities.},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\3GJLUDSM\Elavsky et al. - 2022 - How accessible is my visualization Evaluating vis.pdf}
}

@online{ExperienceLearnEducational,
  title = {Experience + {{Learn}} / {{Educational Media}} / {{Effective Practices}} for {{Description}} of {{Science Content}} within {{Digital Talking Books}} / {{NCAM}}},
  url = {http://ncamftp.wgbh.org/ncam-old-site/experience_learn/educational_media/stemdx.html},
  urldate = {2023-08-28}
}

@article{fanAccessibilityDataVisualizations2022,
  title = {The {{Accessibility}} of {{Data Visualizations}} on the {{Web}} for {{Screen Reader Users}}: {{Practices}} and {{Experiences During COVID-19}}},
  shorttitle = {The {{Accessibility}} of {{Data Visualizations}} on the {{Web}} for {{Screen Reader Users}}},
  author = {Fan, Danyang and Siu, Alexa F. and Rao, Hrishikesh V. and Kim, Gene S-H and Vazquez, Xavier and Greco, Lucy and O’Modhrain, Sile and Follmer, Sean},
  date = {2022-08-18},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  pages = {3557899},
  issn = {1936-7228, 1936-7236},
  doi = {10.1145/3557899},
  url = {https://dl.acm.org/doi/10.1145/3557899},
  urldate = {2023-01-12},
  abstract = {Data visualization has become an increasingly important means of efective data communication and has played a vital role in broadcasting the progression of COVID-19. Accessible data representations, on the other hand, have lagged behind, leaving areas of information out of reach for many blind and visually impaired (BVI) users. In this work, we sought to understand (1) the accessibility of current implementations of visualizations on the web; (2) BVI users’ preferences and current experiences when accessing data-driven media; (3) how accessible data representations on the web address these users’ access needs and help them navigate, interpret, and gain insights from the data; and (4) the practical challenges that limit BVI users’ access and use of data representations. To answer these questions, we conducted a mixed-methods study consisting of an accessibility audit of 87 data visualizations on the web to identify accessibility issues, an online survey of 127 screen reader users to understand lived experiences and preferences, and a remote contextual inquiry with 12 of the survey respondents to observe how they navigate, interpret and gain insights from accessible data representations. Our observations during this critical period of time provide an understanding of the widespread accessibility issues encountered across online data visualizations, the impact that data accessibility inequities have on the BVI community, the ways screen reader users sought access to data-driven information and made use of online visualizations to form insights, and the pressing need to make larger strides towards improving data literacy, building conidence, and enriching methods of access. Based on our indings, we provide recommendations for researchers and practitioners to broaden data accessibility on the web. CCS Concepts: · Human-centered computing → Empirical studies in accessibility; Visualization application domains.},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\Y6Q8L9IR\Fan et al. - 2022 - The Accessibility of Data Visualizations on the We.pdf}
}

@article{fanAccessibilityDataVisualizations2023,
  title = {The {{Accessibility}} of {{Data Visualizations}} on the {{Web}} for {{Screen Reader Users}}: {{Practices}} and {{Experiences During COVID-19}}},
  shorttitle = {The {{Accessibility}} of {{Data Visualizations}} on the {{Web}} for {{Screen Reader Users}}},
  author = {Fan, Danyang and Fay Siu, Alexa and Rao, Hrishikesh and Kim, Gene Sung-Ho and Vazquez, Xavier and Greco, Lucy and O'Modhrain, Sile and Follmer, Sean},
  date = {2023-03-29},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume = {16},
  number = {1},
  pages = {4:1--4:29},
  issn = {1936-7228},
  doi = {10.1145/3557899},
  url = {https://dl.acm.org/doi/10.1145/3557899},
  urldate = {2023-08-27},
  abstract = {Data visualization has become an increasingly important means of effective data communication and has played a vital role in broadcasting the progression of COVID-19. Accessible data representations, however, have lagged behind, leaving areas of information out of reach for many blind and visually impaired (BVI) users. In this work, we sought to understand (1) the accessibility of current implementations of visualizations on the web; (2) BVI users’ preferences and current experiences when accessing data-driven media; (3) how accessible data representations on the web address these users’ access needs and help them navigate, interpret, and gain insights from the data; and (4) the practical challenges that limit BVI users’ access and use of data representations. To answer these questions, we conducted a mixed-methods study consisting of an accessibility audit of 87 data visualizations on the web to identify accessibility issues, an online survey of 127 screen reader users to understand lived experiences and preferences, and a remote contextual inquiry with 12 of the survey respondents to observe how they navigate, interpret, and gain insights from accessible data representations. Our observations during this critical period of time provide an understanding of the widespread accessibility issues encountered across online data visualizations, the impact that data accessibility inequities have on the BVI community, the ways screen reader users sought access to data-driven information and made use of online visualizations to form insights, and the pressing need to make larger strides towards improving data literacy, building confidence, and enriching methods of access. Based on our findings, we provide recommendations for researchers and practitioners to broaden data accessibility on the web.},
  keywords = {Accessibility,accessible data visualization,audit,blind,data visualization,user experience,visually impaired,web accessibility},
  file = {C:\Users\jseo1005\Zotero\storage\L48LCCW3\Fan et al. - 2023 - The Accessibility of Data Visualizations on the We.pdf}
}

@inproceedings{fanSlideToneTiltTone1DOF2022,
  title = {Slide-{{Tone}} and {{Tilt-Tone}}: 1-{{DOF Haptic Techniques}} for {{Conveying Shape Characteristics}} of {{Graphs}} to {{Blind Users}}},
  shorttitle = {Slide-{{Tone}} and {{Tilt-Tone}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Fan, Danyang and Siu, Alexa Fay and Law, Wing-Sum Adrienne and Zhen, Raymond Ruihong and O'Modhrain, Sile and Follmer, Sean},
  date = {2022-04-29},
  pages = {1--19},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3517790},
  url = {https://dl.acm.org/doi/10.1145/3491102.3517790},
  urldate = {2023-01-12},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\KTLG8G4D\Fan et al. - 2022 - Slide-Tone and Tilt-Tone 1-DOF Haptic Techniques .pdf}
}

@inproceedings{farihaMiningFrequentPatterns2013,
  title = {Mining {{Frequent Patterns}} from {{Human Interactions}} in {{Meetings Using Directed Acyclic Graphs}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Fariha, Anna and Ahmed, Chowdhury Farhan and Leung, Carson Kai-Sang and Abdullah, S. M. and Cao, Longbing},
  editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  date = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {38--49},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37453-1_4},
  abstract = {In modern life, interactions between human beings frequently occur in meetings, where topics are discussed. Semantic knowledge of meetings can be revealed by discovering interaction patterns from these meetings. An existing method mines interaction patterns from meetings using tree structures. However, such a tree-based method may not capture all kinds of triggering relations between interactions, and it may not distinguish a participant of a certain rank from another participant of a different rank in a meeting. Hence, the tree-based method may not be able to find all interaction patterns such as those about correlated interaction. In this paper, we propose to mine interaction patterns from meetings using an alternative data structure—namely, a directed acyclic graph (DAG). Specifically, a DAG captures both temporal and triggering relations between interactions in meetings. Moreover, to distinguish one participant of a certain rank from another, we assign weights to nodes in the DAG. As such, a meeting can be modeled as a weighted DAG, from which weighted frequent interaction patterns can be discovered. Experimental results showed the effectiveness of our proposed DAG-based method for mining interaction patterns from meetings.},
  isbn = {978-3-642-37453-1},
  langid = {english},
  keywords = {Data mining,Directed Acyclic Graphs,Frequent patterns,Human interaction,Modeling meetings},
  file = {C:\Users\jseo1005\Zotero\storage\M2HRTAMH\Fariha et al. - 2013 - Mining Frequent Patterns from Human Interactions i.pdf}
}

@inproceedings{fitzpatrickProducingAccessibleStatistics2017,
  title = {Producing {{Accessible Statistics Diagrams}} in {{R}}},
  booktitle = {Proceedings of the 14th {{International Web}} for {{All Conference}}},
  author = {Fitzpatrick, Donal and Godfrey, A. Jonathan R. and Sorge, Volker},
  date = {2017-04-02},
  pages = {1--4},
  publisher = {{ACM}},
  location = {{Perth Western Australia Australia}},
  doi = {10.1145/3058555.3058564},
  url = {https://dl.acm.org/doi/10.1145/3058555.3058564},
  urldate = {2022-08-21},
  abstract = {Blind people are at risk of being left behind in the infor­ mation age if efforts are not made to improve the access to information that is not traditionally conveyed in text, whether that text be accessed in braille, audio, or a com­ puter’s screen reading software. Most graphics summarise a scene or some aspect of data that the author hopes will in­ form their audience; good statistical graphics are commonly used to great effect for the sighted world, but are practi­ cally useless to a blind audience. Our work aims to provide an accessible way for blind users to easily, efficiently, and most importantly accurately, explore and query the data contained in diagrams such as bar charts, box plots, time series, and many more. We employ the statistical software environment R to compute rich semantics for these diagrams and make them web accessible by supporting screen reading and interactive exploration.},
  eventtitle = {{{W4A}} '17: {{Web For All}} 2017 - {{The Future}} of {{Accessible Work}}},
  isbn = {978-1-4503-4900-0},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\ASU5R8I9\Fitzpatrick et al. - 2017 - Producing Accessible Statistics Diagrams in R.pdf}
}

@online{FormInputBindings,
  title = {Form {{Input Bindings}} | {{Vue}}.Js},
  url = {https://vuejs.org/guide/essentials/forms.html#select},
  urldate = {2022-09-24}
}

@inproceedings{gargBraille8UnifiedBraille2016,
  title = {Braille-8 — {{The}} Unified Braille {{Unicode}} System: {{Presenting}} an Ideal Unified System around 8-Dot {{Braille Unicode}} for the Braille Users World-Over},
  shorttitle = {Braille-8 — {{The}} Unified Braille {{Unicode}} System},
  booktitle = {2016 {{IEEE International Conference}} on {{Advanced Networks}} and {{Telecommunications Systems}} ({{ANTS}})},
  author = {Garg, Anupam Kumar},
  date = {2016-11},
  pages = {1--6},
  doi = {10.1109/ANTS.2016.7947839},
  abstract = {Traditional Braille is a 6-dot code that can represent maximum 64 unique symbols with each braille cell. This is grossly insufficient to represent even ordinary English text (comprising 26 small letters, 26 capital letters, 10 digits, and 14 basic punctuations) - let alone math and science symbols. Thus a braille user has to enter 2 (and sometime 3 or 4) braille cells to enter one character or symbol. This makes braille writing very slow and tedious. Incidentally, 8-dot Braille Unicode was introduced to facilitate the Computer Braille that could represent all 95 computer characters with one braille cell itself. Since 8-dot braille can represent maximum 256 unique symbols, it has huge potential to provide the ultimate solution to all woes faced by braille users while writing texts (in English or in native languages) as well as mathematical and technical text. This paper presents a comprehensive unified braille Unicode system providing a detailed mapping of 8-dot braille Unicode pattern to represent the transcribing codes (in English or any other language) as well as the math, science, and computer symbols/characters - mostly with one braille cell itself.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Advanced Networks}} and {{Telecommunications Systems}} ({{ANTS}})},
  keywords = {blind,braille pattern,braille standard,Braille Unicode,braille user,Braille-8,computer braille,Computers,Context,eight-dot braille code,Encoding,Geometry,Set theory,Standards,visually challenged,Writing},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\WE3NJT7N\\Garg - 2016 - Braille-8 — The unified braille Unicode system Pr.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\LBYE5IYN\\authors.html}
}

@inproceedings{giudiceLearningNonvisualGraphical2012,
  title = {Learning Non-Visual Graphical Information Using a Touch-Based Vibro-Audio Interface},
  booktitle = {Proceedings of the 14th International {{ACM SIGACCESS}} Conference on {{Computers}} and Accessibility - {{ASSETS}} '12},
  author = {Giudice, Nicholas A. and Palani, Hari Prasath and Brenner, Eric and Kramer, Kevin M.},
  date = {2012},
  pages = {103},
  publisher = {{ACM Press}},
  location = {{Boulder, Colorado, USA}},
  doi = {10.1145/2384916.2384935},
  url = {http://dl.acm.org/citation.cfm?doid=2384916.2384935},
  urldate = {2022-08-21},
  eventtitle = {The 14th International {{ACM SIGACCESS}} Conference},
  isbn = {978-1-4503-1321-6},
  langid = {english}
}

@inproceedings{godfreyAccessibleInteractionModel2018,
  title = {An {{Accessible Interaction Model}} for {{Data Visualisation}} in {{Statistics}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author = {Godfrey, A. Jonathan R. and Murrell, Paul and Sorge, Volker},
  editor = {Miesenberger, Klaus and Kouroupetroglou, Georgios},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {590--597},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-94277-3_92},
  abstract = {Data is everywhere and its communication and understanding is an important pre-requisite for the full participation of individuals in the information age. Good data visualisation is commonly used to great effect for the sighted world, but are practically useless to a blind audience. Blind people are at risk of being left behind if efforts are not made to improve the access to information that is not traditionally conveyed in text, whether that text be accessed in braille, audio, or a computer’s screen reading software. Our work aims to provide an accessible way for blind users to easily, efficiently, and most importantly accurately, explore and query the data contained in diagrams such as bar charts, box plots, time series, and many more. We employ the statistical software environment R not only as a means to generate accessible diagrams, but also as a way for blind users to directly interact with data in the same way as their sighted peers by supporting immediate data visualisation via screen reading and interactive exploration.},
  isbn = {978-3-319-94277-3},
  langid = {english},
  keywords = {Blind People,Blind Users,Data Visualisation,Screen Reader,Statistical Software Application},
  file = {C:\Users\jseo1005\Zotero\storage\H8QE92CU\Godfrey et al. - 2018 - An Accessible Interaction Model for Data Visualisa.pdf}
}

@article{godfreyAdviceBlindTeachers2015,
  title = {Advice {{From Blind Teachers}} on {{How}} to {{Teach Statistics}} to {{Blind Students}}},
  author = {Godfrey, A. Jonathan R. and Loots, M. Theodor},
  date = {2015-11-01},
  journaltitle = {Journal of Statistics Education},
  volume = {23},
  number = {3},
  pages = {null},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/10691898.2015.11889746},
  url = {https://doi.org/10.1080/10691898.2015.11889746},
  urldate = {2023-01-05},
  abstract = {Blind students are bound to make up a very small part of the population most university lecturers will encounter during their careers. Research to date shows that good communication between staff and student improves the chances of a successful outcome for both parties. The research does show, however, that the exercise seems to be one of re-inventing the wheel, perhaps with a less than fully informed blueprint to work from.The authors use their own experiences as blind students who progressed beyond research methods or first year introductory courses into careers as teachers and researchers of statistical methods to provide guidance for their sighted colleagues. Our principle point of difference to the existing research work is that we rely on the experience of our statistical education for our current livelihoods; we were not one-off students taking a research methodology course or first year introductory course. We benefitted from the successful (and possibly the not so successful) interactions we had with our sighted teachers. It is our hope that by saving staff from wasted effort, we can spare students from unnecessary discomfort in classes that could improve their future employment prospects. Our aim is therefore to provide practical support for our sighted colleagues and blind peers as we work together towards the empowerment of blind students in becoming competent producers of statistical information, not just consumers who interpret that information.},
  keywords = {Braille,Low vision,Speech output,Tactile images},
  file = {C:\Users\jseo1005\Zotero\storage\8GHY4ISY\Godfrey and Loots - 2015 - Advice From Blind Teachers on How to Teach Statist.pdf}
}

@article{godfreyStatisticalSoftwareBlind2013,
  title = {Statistical {{Software}} from a {{Blind Person}}'s {{Perspective}}},
  author = {Godfrey, Jonathan,R., A.},
  date = {2013},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  volume = {5},
  number = {1},
  pages = {73},
  issn = {2073-4859},
  doi = {10.32614/RJ-2013-007},
  url = {https://journal.r-project.org/archive/2013/RJ-2013-007/index.html},
  urldate = {2022-08-22},
  abstract = {Blind people have experienced access issues to many software applications since the advent of the Windows operating system; statistical software has proven to follow the rule and not be an exception. The ability to use R within minutes of download with next to no adaptation has opened doors for accessible production of statistical analyses for this author (himself blind) and blind students around the world. This article shows how little is required to make R the most accessible statistical software available today. There is any number of ramifications that this opportunity creates for blind students, especially in terms of their future research and employment prospects. There is potential for making R even better for blind users. The extensibility of R makes this possible through added functionality being made available in an add-on package called BrailleR. Functions in this package are intended to make graphical information available in text form.},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\HGNVE9WY\Godfrey - 2013 - Statistical Software from a Blind Person's Perspec.pdf}
}

@inproceedings{gotzelmannLucentMaps3DPrinted2016,
  title = {{{LucentMaps}}: {{3D Printed Audiovisual Tactile Maps}} for {{Blind}} and {{Visually Impaired People}}},
  shorttitle = {{{LucentMaps}}},
  booktitle = {Proceedings of the 18th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Götzelmann, Timo},
  date = {2016-10-23},
  series = {{{ASSETS}} '16},
  pages = {81--90},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2982142.2982163},
  url = {https://dl.acm.org/doi/10.1145/2982142.2982163},
  urldate = {2023-09-09},
  abstract = {Tactile maps support blind and visually impaired people in orientation and to familiarize with unfamiliar environments. Interactive approaches complement these maps with auditory feedback. However, commonly these approaches focus on blind people. We present an approach which incorporates visually impaired people by visually augmenting relevant parts of tactile maps. These audiovisual tactile maps can be used in conjunction with common tablet computers and smartphones. By integrating conductive elements into 3D printed tactile maps, they can be recognized by a single touch on the mobile device's display, which eases the handling for blind and visually impaired people. To allow multiple elevation levels in our transparent tactile maps, we conducted a study to reconcile technical and physiological requirements of off-the-shelf 3D printers, capacitive touch inputs and the human tactile sense. We propose an interaction concept for 3D printed audiovisual tactile maps, verify its feasibility and test it with a user study. Our discussion includes economic considerations crucial for a broad dissemination of tactile maps for both blind and visually impaired people.},
  isbn = {978-1-4503-4124-0},
  keywords = {3d printing,accessibility,audio-tactile,blind,capacitive,capacitive sensing,functional,global,marker,orientation,tactile maps,tangible user interfaces,touch screen},
  file = {C:\Users\jseo1005\Zotero\storage\CE9AZRH9\Götzelmann - 2016 - LucentMaps 3D Printed Audiovisual Tactile Maps fo.pdf}
}

@article{gotzelmannVisuallyAugmentedAudioTactile2018,
  title = {Visually {{Augmented Audio-Tactile Graphics}} for {{Visually Impaired People}}},
  author = {Götzelmann, T.},
  date = {2018-06-08},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume = {11},
  number = {2},
  pages = {8:1--8:31},
  issn = {1936-7228},
  doi = {10.1145/3186894},
  url = {https://dl.acm.org/doi/10.1145/3186894},
  urldate = {2023-09-09},
  abstract = {Tactile graphics play an essential role in knowledge transfer for blind people. The tactile exploration of these graphics is often challenging because of the cognitive load caused by physiological constraints and their complexity. The coupling of physical tactile graphics with electronic devices offers to support the tactile exploration by auditory feedback. Often, these systems have strict constraints regarding their mobility or the process of coupling both components. Additionally, visually impaired people cannot appropriately benefit from their residual vision. This article presents a concept for 3D printed tactile graphics, which offers to use audio-tactile graphics with usual smartphones or tablet-computers. By using capacitive markers, the coupling of the tactile graphics with the mobile device is simplified. These tactile graphics integrating these markers can be printed in one turn by off-the-shelf 3D printers without any post-processing and allows us to use multiple elevation levels for graphical elements. Based on the developed generic concept on visually augmented audio-tactile graphics, we presented a case study for maps. A prototypical implementation was tested by a user study with visually impaired people. All the participants were able to interact with the 3D printed tactile maps using a standard tablet computer. To study the effect of visual augmentation of graphical elements, we conducted another comprehensive user study. We tested multiple types of graphics and obtained evidence that visual augmentation may offer clear advantages for the exploration of tactile graphics. Even participants with a minor residual vision could solve the tasks with visual augmentation more quickly and accurately.},
  keywords = {3D printing,accessibility,audio-tactile,augmented,blind,capacitive,capacitive sensing,global,marker,orientation,Tactile graphics,tangible user interfaces,touch screen,visually impaired},
  file = {C:\Users\jseo1005\Zotero\storage\6T38EXCR\Götzelmann - 2018 - Visually Augmented Audio-Tactile Graphics for Visu.pdf}
}

@online{gouldEffectivePracticesDescription2008,
  title = {Effective {{Practices}} for {{Description}} of {{Science Content}} within {{Digital Talking Books}}},
  author = {Gould, Bryan and O'Connell, Trisha and Freed, Geoff},
  date = {2008-12},
  url = {http://ncamftp.wgbh.org/ncam-old-site/experience_learn/educational_media/stemdx.html},
  urldate = {2023-08-27},
  file = {C:\Users\jseo1005\Zotero\storage\UL4K86A9\Experience + Learn  Educational Media  Effective Practices for Description of Science Content with.pdf}
}

@incollection{hartDevelopmentNASATLXTask1988,
  title = {Development of {{NASA-TLX}} ({{Task Load Index}}): {{Results}} of {{Empirical}} and {{Theoretical Research}}},
  shorttitle = {Development of {{NASA-TLX}} ({{Task Load Index}})},
  booktitle = {Advances in {{Psychology}}},
  author = {Hart, Sandra G. and Staveland, Lowell E.},
  editor = {Hancock, Peter A. and Meshkati, Najmedin},
  date = {1988-01-01},
  series = {Human {{Mental Workload}}},
  volume = {52},
  pages = {139--183},
  publisher = {{North-Holland}},
  doi = {10.1016/S0166-4115(08)62386-9},
  url = {https://www.sciencedirect.com/science/article/pii/S0166411508623869},
  urldate = {2023-03-19},
  abstract = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\G9FV42GA\\Hart and Staveland - 1988 - Development of NASA-TLX (Task Load Index) Results.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\C4YNUMGI\\S0166411508623869.html}
}

@software{hassakuAudioplotlib2022,
  title = {Audio-Plot-Lib},
  author = {{hassaku}},
  date = {2022-07-19T09:41:21Z},
  origdate = {2020-12-13T07:16:03Z},
  url = {https://github.com/hassaku/audio-plot-lib},
  urldate = {2022-08-21},
  abstract = {This library provides graph sonification functions and has been developed for a project named "Data science and machine learning resources for screen reader users". Please refer to the project page for more details.},
  keywords = {audio,data-science,google-colab,graphs,machine-learning,python,sonification,visually-impaired}
}

@inproceedings{heTacTILEPreliminaryToolchain2017,
  title = {{{TacTILE}}: {{A Preliminary Toolchain}} for {{Creating Accessible Graphics}} with {{3D-Printed Overlays}} and {{Auditory Annotations}}},
  shorttitle = {{{TacTILE}}},
  booktitle = {Proceedings of the 19th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {He, Liang and Wan, Zijian and Findlater, Leah and Froehlich, Jon E.},
  date = {2017-10-19},
  series = {{{ASSETS}} '17},
  pages = {397--398},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3132525.3134818},
  url = {https://dl.acm.org/doi/10.1145/3132525.3134818},
  urldate = {2023-09-09},
  abstract = {Tactile overlays with audio annotations can increase the accessibility of touchscreens for blind users; however, preparing these overlays is complex and labor intensive. We introduce TacTILE, a novel toolchain to more easily create tactile overlays with audio annotations for arbitrary touchscreen graphics (e.g., graphs, pictures, maps). The workflow includes: (i) an annotation tool to add audio to graphical elements, (ii) a fabrication process that generates 3D-printed tactile overlays, and (iii) a custom app for the user to explore graphics with these overlays. We close with a pilot study with one blind participant who explores three examples (floor plan, photo, and chart), and a discussion of future work.},
  isbn = {978-1-4503-4926-0},
  keywords = {3d printing,accessible graphics,blind users,speech,tactile overlays,touchscreens.,visual impairments},
  file = {C:\Users\jseo1005\Zotero\storage\T355EZW5\He et al. - 2017 - TacTILE A Preliminary Toolchain for Creating Acce.pdf}
}

@software{HighchartsHighcharts2022,
  title = {Highcharts/Highcharts},
  date = {2022-08-21T14:58:48Z},
  origdate = {2010-06-11T12:23:53Z},
  url = {https://github.com/highcharts/highcharts},
  urldate = {2022-08-21},
  abstract = {Highcharts JS, the JavaScript charting framework},
  organization = {{Highcharts}}
}

@software{HighchartsSonificationStudio2022,
  title = {Highcharts {{Sonification Studio}}},
  date = {2022-11-18T10:46:22Z},
  origdate = {2019-08-26T11:20:08Z},
  url = {https://github.com/highcharts/sonification-studio},
  urldate = {2023-03-11},
  organization = {{Highcharts}}
}

@inproceedings{hollowayAnimationsYourFingertips2022,
  title = {Animations at {{Your Fingertips}}: {{Using}} a {{Refreshable Tactile Display}} to {{Convey Motion Graphics}} for {{People}} Who Are {{Blind}} or Have {{Low Vision}}},
  shorttitle = {Animations at {{Your Fingertips}}},
  booktitle = {The 24th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Holloway, Leona and Ananthanarayan, Swamy and Butler, Matthew and De Silva, Madhuka Thisuri and Ellis, Kirsten and Goncu, Cagatay and Stephens, Kate and Marriott, Kim},
  date = {2022-10-22},
  pages = {1--16},
  publisher = {{ACM}},
  location = {{Athens Greece}},
  doi = {10.1145/3517428.3544797},
  url = {https://dl.acm.org/doi/10.1145/3517428.3544797},
  urldate = {2023-09-07},
  eventtitle = {{{ASSETS}} '22: {{The}} 24th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  isbn = {978-1-4503-9258-7},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\HWAUDDT7\Holloway et al. - 2022 - Animations at Your Fingertips Using a Refreshable Tactile Display to Convey Motion Graphics for Peo.pdf}
}

@article{hooperDesigningMoreEffective2011,
  title = {Towards Designing More Effective Systems by Understanding User Experiences},
  author = {Hooper, Clare J.},
  date = {2011-09},
  journaltitle = {ACM SIGWEB Newsletter},
  shortjournal = {SIGWEB Newsl.},
  pages = {1--3},
  issn = {1931-1745, 1931-1435},
  doi = {10.1145/2020936.2020940},
  url = {https://dl.acm.org/doi/10.1145/2020936.2020940},
  urldate = {2022-08-21},
  abstract = {This work is about social technologies, user experiences and the problems of creative design. It is motivated by a desire to give people who are offline --- whether for reasons of poverty, disability, infrastructure or cultural background --- the access to social technologies that is currently provided via the web, letting them access the online content and communication facilities that so many of us take for granted. There exist simple technologically-oriented approaches to this problem, such as identifying functional requirements and prototyping tools. This focus on technology, however, comes at a cost of neglecting the experiential aspects which motivate the work, and can result in systems that are functional but unappealing to (or even unusable by) their target audiences.},
  issue = {Autumn},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\JF48F5CD\Hooper - 2011 - Towards designing more effective systems by unders.pdf}
}

@inproceedings{hoqueAccessibleDataRepresentation2023,
  title = {Accessible {{Data Representation}} with {{Natural Sound}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hoque, Md Naimul and Ehtesham-Ul-Haque, Md and Elmqvist, Niklas and Billah, Syed Masum},
  date = {2023-04-19},
  series = {{{CHI}} '23},
  pages = {1--19},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581087},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581087},
  urldate = {2023-09-10},
  abstract = {Sonification translates data into non-speech audio. Such auditory representations can make data visualization accessible to people who are blind or have low vision (BLV). This paper presents a sonification method for translating common data visualization into a blend of natural sounds. We hypothesize that people’s familiarity with sounds drawn from nature, such as birds singing in a forest, and their ability to listen to these sounds in parallel, will enable BLV users to perceive multiple data points being sonified at the same time. Informed by an extensive literature review and a preliminary study with 5 BLV participants, we designed an accessible data representation tool, Susurrus, that combines our sonification method with other accessibility features, such as keyboard interaction and text-to-speech feedback. Finally, we conducted a user study with 12 BLV participants and report the potential and application of natural sounds for sonification compared to existing sonification tools.},
  isbn = {978-1-4503-9421-5},
  keywords = {Accessibility,Data visualization,Natural sound,Sonification},
  file = {C:\Users\jseo1005\Zotero\storage\Q5SR9E3T\Hoque et al. - 2023 - Accessible Data Representation with Natural Sound.pdf}
}

@article{hunterMatplotlib2DGraphics2007,
  title = {Matplotlib: {{A 2D Graphics Environment}}},
  shorttitle = {Matplotlib},
  author = {Hunter, John D.},
  date = {2007-05-01},
  journaltitle = {Computing in Science \& Engineering},
  volume = {9},
  number = {03},
  pages = {90--95},
  publisher = {{IEEE Computer Society}},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2007.55},
  url = {https://www.computer.org/csdl/magazine/cs/2007/03/c3090/13rRUwbJD0A},
  urldate = {2023-01-24},
  abstract = {Matplotlib is a 2D graphics package for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
  langid = {english}
}

@article{huntInteractiveSonification2011,
  title = {Interactive {{Sonification}}},
  author = {Hunt, Andy and Hermann, Thomas},
  date = {2011},
  journaltitle = {The Sonification Handbook},
  url = {https://pub.uni-bielefeld.de/record/2935181},
  urldate = {2023-09-09},
  abstract = {This chapter places a special focus on those situations where there is a tight control loop (a real-time interactive collaboration) between the human user and the system producing the sonification. It explains the background (why humans appear to use interactive sonification as a natural tool for exploring the world) as well as describing the different methods and application domains.},
  isbn = {9783832528195},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\ERJKZBBJ\2935181.html}
}

@inproceedings{joynerVisualizationAccessibilityWild2022,
  title = {Visualization {{Accessibility}} in the {{Wild}}: {{Challenges Faced}} by {{Visualization Designers}}},
  shorttitle = {Visualization {{Accessibility}} in the {{Wild}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Joyner, Shakila Cherise S and Riegelhuth, Amalia and Garrity, Kathleen and Kim, Yea-Seul and Kim, Nam Wook},
  date = {2022-04-27},
  pages = {1--19},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3517630},
  url = {https://dl.acm.org/doi/10.1145/3491102.3517630},
  urldate = {2022-12-29},
  abstract = {Data visualizations are now widely used across many disciplines. However, many of them are not easily accessible for visually impaired people. In this work, we use three-staged mixed methods to understand the current practice of accessible visualization design for visually impaired people. We analyzed 95 visualizations from various venues to inspect how they are made inaccessible. To understand the rationale and context behind the design choices, we also conducted surveys with 144 practitioners in the U.S. and follow-up interviews with ten selected survey participants. Our findings include the difficulties of handling modern complex and interactive visualizations and the lack of accessibility support from visualization tools in addition to personal and organizational factors making it challenging to perform accessible design practices.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\B63HSLUG\Joyner et al. - 2022 - Visualization Accessibility in the Wild Challenge.pdf}
}

@software{julianna-langstonChart2Music2022,
  title = {{{Chart2Music}}},
  author = {family=langston, prefix=julianna-, useprefix=true},
  date = {2022-08-15T04:28:31Z},
  origdate = {2022-06-12T02:59:46Z},
  url = {https://github.com/julianna-langston/chart2music},
  urldate = {2022-08-20},
  abstract = {Turns charts into music so the blind can hear data}
}

@inproceedings{kadayatImpactSentenceLength2020,
  title = {Impact of {{Sentence Length}} on the {{Readability}} of {{Web}} for {{Screen Reader Users}}},
  booktitle = {Universal {{Access}} in {{Human-Computer Interaction}}. {{Design Approaches}} and {{Supporting Technologies}}},
  author = {Kadayat, Bam Bahadur and Eika, Evelyn},
  editor = {Antona, Margherita and Stephanidis, Constantine},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {261--271},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-49282-3_18},
  abstract = {Readability of text is generally believed to be connected to sentence length. Most studies on readability are based on visual reading. Less is known about text readability for users relying on screen readers, such as users who are blind. This study therefore set out to investigate the effect of sentence length on the readability of web texts accessed using screen readers. A controlled within-subjects experiment was performed with twenty-one participants. Participants used a screen reader to read five texts with different sentence lengths. The participants’ comprehension and perceived workload were measured. The findings reveal that there is a significant effect of sentence length and most participants exhibit the highest comprehension and lowest workload with sentences comprising 16–20 words. Implications of these results are that web content providers should strive for sentence length of 16–20 words to maximize readability.},
  isbn = {978-3-030-49282-3},
  langid = {english},
  keywords = {Accessibility,Blind,Readability,Screen reader,Sentence length,Universal design,Workload},
  file = {C:\Users\jseo1005\Zotero\storage\RG3JKF76\Kadayat and Eika - 2020 - Impact of Sentence Length on the Readability of We.pdf}
}

@inproceedings{kierasGeneralizedTransitionNetwork1983,
  title = {A Generalized Transition Network Representation for Interactive Systems},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kieras, David and Polson, Peter G.},
  date = {1983-12-12},
  series = {{{CHI}} '83},
  pages = {103--106},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/800045.801590},
  url = {https://dl.acm.org/doi/10.1145/800045.801590},
  urldate = {2023-09-07},
  abstract = {A general method for describing the behavior of an interactive system is presented which is based on transition networks generalized enough to describe even very complex systems easily, as shown by an example description of a word processor. The key feature is the ability to easily describe hierarchies of modes or states of the system. The representation system is especially valuable as a design tool when used in a simulation of a proposed user interface. In order to characterize the interaction between a user and a system, an explicit and formal representation of the behavior of the system itself is needed. To be of value in the design of user interfaces, the representation should be independent of the actual implementation of the system, but also reflect the structural properties of the system's behavior, such as its hierarchical form, the possible modes, and the consistent patterns of interaction. At the same time, the representation must be easy to define and understand. This paper presents a representation notation with these properties.},
  isbn = {978-0-89791-121-4},
  file = {C:\Users\jseo1005\Zotero\storage\TGI29GKW\Kieras and Polson - 1983 - A generalized transition network representation for interactive systems.pdf}
}

@article{kimAccessibleVisualizationDesign2021,
  title = {Accessible {{Visualization}}: {{Design Space}}, {{Opportunities}}, and {{Challenges}}},
  shorttitle = {Accessible {{Visualization}}},
  author = {Kim, N. W. and Joyner, S. C. and Riegelhuth, A. and Kim, Y.},
  date = {2021-06},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {40},
  number = {3},
  pages = {173--188},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.14298},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14298},
  urldate = {2022-02-09},
  abstract = {Visualizations are now widely used across disciplines to understand and communicate data. The benefit of visualizations lies in leveraging our natural visual perception. However, the sole dependency on vision can produce unintended discrimination against people with visual impairments. While the visualization field has seen enormous growth in recent years, supporting people with disabilities is much less explored. In this work, we examine approaches to support this marginalized user group, focusing on visual disabilities. We collected and analyzed papers published for the last 20 years on visualization accessibility. We mapped a design space for accessible visualization that includes seven dimensions: user group, literacy task, chart type, interaction, information granularity, sensory modality, assistive technology. We described the current knowledge gap in light of the latest advances in visualization and presented a preliminary accessibility model by synthesizing findings from existing research. Finally, we reflected on the dimensions and discussed opportunities and challenges for future research.},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\E42LXLJL\Kim et al. - 2021 - Accessible Visualization Design Space, Opportunit.pdf}
}

@inproceedings{kimAnsweringQuestionsCharts2020,
  title = {Answering {{Questions}} about {{Charts}} and {{Generating Visual Explanations}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kim, Dae Hyun and Hoque, Enamul and Agrawala, Maneesh},
  date = {2020-04-21},
  pages = {1--13},
  publisher = {{ACM}},
  location = {{Honolulu HI USA}},
  doi = {10.1145/3313831.3376467},
  url = {https://dl.acm.org/doi/10.1145/3313831.3376467},
  urldate = {2022-08-21},
  eventtitle = {{{CHI}} '20: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-6708-0},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\PB8PFLZX\Kim et al. - 2020 - Answering Questions about Charts and Generating Vi.pdf}
}

@inproceedings{kimExploringChartQuestion2023,
  title = {Exploring {{Chart Question Answering}} for {{Blind}} and {{Low Vision Users}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kim, Jiho and Srinivasan, Arjun and Kim, Nam Wook and Kim, Yea-Seul},
  date = {2023-04-19},
  series = {{{CHI}} '23},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581532},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581532},
  urldate = {2023-05-05},
  abstract = {Data visualizations can be complex or involve numerous data points, making them impractical to navigate using screen readers alone. Question answering (QA) systems have the potential to support visualization interpretation and exploration without overwhelming blind and low vision (BLV) users. To investigate if and how QA systems can help BLV users in working with visualizations, we conducted a Wizard of Oz study with 24 BLV people where participants freely posed queries about four visualizations. We collected 979 queries and mapped them to popular analytic task taxonomies. We found that retrieving value and finding extremum were the most common tasks, participants often made complex queries and used visual references, and the data topic notably influenced the queries. We compile a list of design considerations for accessible chart QA systems and make our question corpus publicly available to guide future research and development.},
  isbn = {978-1-4503-9421-5},
  keywords = {Accessibility,Design Considerations,Human-Subjects Qualitative Studies,Question Answering,Visualization},
  file = {C:\Users\jseo1005\Zotero\storage\XWHZ35QQ\Kim et al. - 2023 - Exploring Chart Question Answering for Blind and L.pdf}
}

@book{kressMultimodalitySocialSemiotic2010,
  title = {Multimodality: {{A Social Semiotic Approach}} to {{Contemporary Communication}}},
  shorttitle = {Multimodality},
  author = {Kress, Gunther R.},
  date = {2010},
  eprint = {ihTm_cI58JQC},
  eprinttype = {googlebooks},
  publisher = {{Taylor \& Francis}},
  abstract = {The 21st century is awash with ever more mixed and remixed images, writing, layout, sound, gesture, speech, and 3D objects. Multimodality looks beyond language and examines these multiple modes of communication and meaning making.   Multimodality: A Social Semiotic Approach to Contemporary Communication represents a long-awaited and much anticipated addition to the study of multimodality from the scholar who pioneered and continues to play a decisive role in shaping the field. Written in an accessible manner and illustrated with a wealth of photos and illustrations to clearly demonstrate the points made, Multimodality: A Social Semiotic Approach to Contemporary Communication deliberately sets out to locate communication in the everyday, covering topics and issues not usually discussed in books of this kind, from traffic signs to mobile phones.   In this book, Gunther Kress presents a contemporary, distinctive and widely applicable approach to communication. He provides the framework necessary for understanding the attempt to bring all modes of meaning-making together under one unified theoretical roof.  This exploration of an increasingly vital area of language and communication studies will be of interest to advanced undergraduate and postgraduate students in the fields of English language and applied linguistics, media and communication studies and education.},
  isbn = {978-0-415-32060-3},
  langid = {english},
  pagetotal = {234},
  keywords = {Language Arts \& Disciplines / Communication Studies}
}

@article{krossDemocratizationDataScience2020,
  title = {The {{Democratization}} of {{Data Science Education}}},
  author = {Kross, Sean and Peng, Roger D. and Caffo, Brian S. and Gooding, Ira and Leek, Jeffrey T.},
  date = {2020-01-02},
  journaltitle = {The American Statistician},
  volume = {74},
  number = {1},
  pages = {1--7},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1668849},
  url = {https://doi.org/10.1080/00031305.2019.1668849},
  urldate = {2023-01-05},
  abstract = {Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.},
  keywords = {Applications and case studies,Education,Statistical computing}
}

@article{leeDataUseMiddle2018,
  title = {Data {{Use}} by {{Middle}} and {{Secondary Students}} in the {{Digital Age}}: {{A Status Report}} and {{Future Prospects}}},
  shorttitle = {Data {{Use}} by {{Middle}} and {{Secondary Students}} in the {{Digital Age}}},
  author = {Lee, Victor and Wilkerson, Michelle},
  date = {2018-01-01},
  journaltitle = {Instructional Technology and Learning Sciences Faculty Publications},
  pages = {1--43},
  url = {https://digitalcommons.usu.edu/itls_facpub/634},
  file = {C:\Users\jseo1005\Zotero\storage\PR22UQB9\634.html}
}

@article{leeHowPeopleMake2016,
  title = {How Do {{People Make Sense}} of {{Unfamiliar Visualizations}}?: {{A Grounded Model}} of {{Novice}}'s {{Information Visualization Sensemaking}}},
  shorttitle = {How Do {{People Make Sense}} of {{Unfamiliar Visualizations}}?},
  author = {Lee, Sukwon and Kim, Sung-Hee and Hung, Ya-Hsin and Lam, Heidi and Kang, Youn-Ah and Yi, Ji Soo},
  date = {2016-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {22},
  number = {1},
  pages = {499--508},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2015.2467195},
  abstract = {In this paper, we would like to investigate how people make sense of unfamiliar information visualizations. In order to achieve the research goal, we conducted a qualitative study by observing 13 participants when they endeavored to make sense of three unfamiliar visualizations (i.e., a parallel-coordinates plot, a chord diagram, and a treemap) that they encountered for the first time. We collected data including audio/video record of think-aloud sessions and semi-structured interview; and analyzed the data using the grounded theory method. The primary result of this study is a grounded model of NOvice's information VIsualization Sensemaking (NOVIS model), which consists of the five major cognitive activities: 1 encountering visualization, 2 constructing a frame, 3 exploring visualization, 4 questioning the frame, and 5 floundering on visualization. We introduce the NOVIS model by explaining the five activities with representative quotes from our participants. We also explore the dynamics in the model. Lastly, we compare with other existing models and share further research directions that arose from our observations.},
  eventtitle = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  keywords = {Data visualization,Encoding,grounded theory,Hidden Markov models,Image color analysis,information visualization,Interviews,novice users,qualitative study,Sensemaking model,Vehicles,Visualization},
  file = {C:\Users\jseo1005\Zotero\storage\5U3YM9W5\Lee et al. - 2016 - How do People Make Sense of Unfamiliar Visualizati.pdf}
}

@article{leeReachingBroaderAudiences2020,
  title = {Reaching {{Broader Audiences With Data Visualization}}},
  author = {Lee, Bongshin and Choe, Eun Kyoung and Isenberg, Petra and Marriott, Kim and Stasko, John},
  date = {2020-03-01},
  journaltitle = {IEEE Computer Graphics and Applications},
  shortjournal = {IEEE Comput. Grap. Appl.},
  volume = {40},
  number = {2},
  pages = {82--90},
  issn = {0272-1716, 1558-1756},
  doi = {10.1109/MCG.2020.2968244},
  url = {https://ieeexplore.ieee.org/document/9023497/},
  urldate = {2023-01-12},
  abstract = {The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this paper, we discuss four research topics—personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization—that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\M7C3NLRW\Lee et al. - 2020 - Reaching Broader Audiences With Data Visualization.pdf}
}

@inproceedings{liuCrossA11yIdentifyingVideo2022,
  title = {{{CrossA11y}}: {{Identifying Video Accessibility Issues}} via {{Cross-modal Grounding}}},
  shorttitle = {{{CrossA11y}}},
  booktitle = {The 35th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Liu, Xingyu "Bruce" and Wang, Ruolin and Li, Dingzeyu and Chen, Xiang 'Anthony' and Pavel, Amy},
  date = {2022-10-29},
  eprint = {2208.11144},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--14},
  doi = {10.1145/3526113.3545703},
  url = {http://arxiv.org/abs/2208.11144},
  urldate = {2023-01-10},
  abstract = {Authors make their videos visually accessible by adding audio descriptions (AD), and auditorily accessible by adding closed captions (CC). However, creating AD and CC is challenging and tedious, especially for non-professional describers and captioners, due to the difficulty of identifying accessibility problems in videos. A video author will have to watch the video through and manually check for inaccessible information frame-by-frame, for both visual and auditory modalities. In this paper, we present CrossA11y, a system that helps authors efficiently detect and address visual and auditory accessibility issues in videos. Using cross-modal grounding analysis, CrossA11y automatically measures accessibility of visual and audio segments in a video by checking for modality asymmetries. CrossA11y then displays these segments and surfaces visual and audio accessibility issues in a unified interface, making it intuitive to locate, review, script AD/CC in-place, and preview the described and captioned video immediately. We demonstrate the effectiveness of CrossA11y through a lab study with 11 participants, comparing to existing baseline.},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\XF5LI8D7\\Liu et al. - 2022 - CrossA11y Identifying Video Accessibility Issues .pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\9V9UW4AL\\2208.html}
}

@article{lundgardAccessibleVisualizationNatural2022,
  title = {Accessible {{Visualization}} via {{Natural Language Descriptions}}: {{A Four-Level Model}} of {{Semantic Content}}},
  shorttitle = {Accessible {{Visualization}} via {{Natural Language Descriptions}}},
  author = {Lundgard, Alan and Satyanarayan, Arvind},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  volume = {28},
  number = {1},
  pages = {1073--1083},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2021.3114770},
  url = {https://ieeexplore.ieee.org/document/9555469/},
  urldate = {2022-07-28},
  abstract = {Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization. Index Terms—Visualization, natural language, description, caption, semantic, model, theory, alt text, blind, disability, accessibility.},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\QTJXT9NN\Lundgard and Satyanarayan - 2022 - Accessible Visualization via Natural Language Desc.pdf}
}

@inproceedings{mackWhatWeMean2021,
  title = {What {{Do We Mean}} by “{{Accessibility Research}}”?: {{A Literature Survey}} of {{Accessibility Papers}} in {{CHI}} and {{ASSETS}} from 1994 to 2019},
  shorttitle = {What {{Do We Mean}} by “{{Accessibility Research}}”?},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Mack, Kelly and McDonnell, Emma and Jain, Dhruv and Lu Wang, Lucy and E. Froehlich, Jon and Findlater, Leah},
  date = {2021-05-06},
  pages = {1--18},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445412},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445412},
  urldate = {2023-05-17},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\6CJFVKZF\Mack et al. - 2021 - What Do We Mean by “Accessibility Research” A Li.pdf}
}

@article{marriottInclusiveDataVisualization2021,
  title = {Inclusive Data Visualization for People with Disabilities: A Call to Action},
  shorttitle = {Inclusive Data Visualization for People with Disabilities},
  author = {Marriott, Kim and Lee, Bongshin and Butler, Matthew and Cutrell, Ed and Ellis, Kirsten and Goncu, Cagatay and Hearst, Marti and McCoy, Kathleen and Szafir, Danielle Albers},
  date = {2021-04-27},
  journaltitle = {Interactions},
  shortjournal = {interactions},
  volume = {28},
  number = {3},
  pages = {47--51},
  issn = {1072-5520},
  doi = {10.1145/3457875},
  url = {https://doi.org/10.1145/3457875},
  urldate = {2022-02-09},
  file = {C:\Users\jseo1005\Zotero\storage\GKVYCZ3W\Marriott et al. - 2021 - Inclusive data visualization for people with disab.pdf}
}

@article{marsonTeachingIntroductoryStatistics2013,
  title = {Teaching Introductory Statistics to Blind Students},
  author = {Marson, Stephen M. and Harrington, Charles F. and Walls, Adam},
  date = {2013},
  journaltitle = {Teaching Statistics},
  volume = {35},
  number = {1},
  pages = {21--25},
  issn = {1467-9639},
  doi = {10.1111/j.1467-9639.2012.00510.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9639.2012.00510.x},
  urldate = {2023-01-05},
  abstract = {The challenges of learning statistics, particularly distributions and their characteristics, can be potentially monumental for vision impaired and blind students. The authors provide some practical advice for teaching these students.},
  langid = {english},
  keywords = {Kinesthetic explanations,Nemeth Code,Teaching,Test autopsy},
  file = {C:\Users\jseo1005\Zotero\storage\MZUIW762\j.1467-9639.2012.00510.html}
}

@incollection{mayerCognitiveTheoryMultimedia2014,
  title = {Cognitive {{Theory}} of {{Multimedia Learning}}},
  booktitle = {The {{Cambridge Handbook}} of {{Multimedia Learning}}},
  author = {Mayer, Richard E.},
  editor = {Mayer, Richard E.},
  date = {2014},
  series = {Cambridge {{Handbooks}} in {{Psychology}}},
  edition = {2},
  pages = {43--71},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9781139547369.005},
  url = {https://www.cambridge.org/core/books/cambridge-handbook-of-multimedia-learning/cognitive-theory-of-multimedia-learning/24E5AEDEC8F4137E37E15BD2BCA91326},
  urldate = {2022-08-19},
  abstract = {AbstractA fundamental hypothesis underlying research on multimedia learning is that multimedia instructional messages that are designed in light of how the human mind works are more likely to lead to meaningful learning than those that are not so designed. The cognitive theory of multimedia learning is based on three cognitive science principles of learning: the human information processing system includes dual channels for visual/pictorial and auditory/verbal processing (i.e., dual-channel assumption), each channel has a limited capacity for processing (i.e., limited-capacity assumption), and active learning entails carrying out a coordinated set of cognitive processes during learning (i.e., active processing assumption). The cognitive theory of multimedia learning specifies five cognitive processes in multimedia learning: selecting relevant words from the presented text or narration, selecting relevant images from the presented graphics, organizing the selected words into a coherent verbal representation, organizing selected images into a coherent pictorial representation, and integrating the pictorial and verbal representations and prior knowledge. Three demands on the learner’s cognitive capacity during learning are extraneous processing (which is not related to the instructional objective), essential processing (which is needed to mentally represent the essential material as presented), and generative processing (which is aimed at making sense of the material). Three instructional goals are to reduce extraneous processing (for extraneous overload situations), manage essential processing (for essential overload situations), and foster generative processing (for generative underuse situations). Multimedia instructional messages should be designed to guide appropriate cognitive processing during learning without overloading the learner’s cognitive system.},
  isbn = {978-1-139-54736-9},
  file = {C:\Users\jseo1005\Zotero\storage\RCS6J7M7\24E5AEDEC8F4137E37E15BD2BCA91326.html}
}

@inproceedings{mcgookinSoundBarExploitingMultiple2006,
  title = {{{SoundBar}}: Exploiting Multiple Views in Multimodal Graph Browsing},
  shorttitle = {{{SoundBar}}},
  booktitle = {Proceedings of the 4th {{Nordic}} Conference on {{Human-computer}} Interaction Changing Roles - {{NordiCHI}} '06},
  author = {McGookin, David K. and Brewster, Stephen A.},
  date = {2006},
  pages = {145--154},
  publisher = {{ACM Press}},
  location = {{Oslo, Norway}},
  doi = {10.1145/1182475.1182491},
  url = {http://portal.acm.org/citation.cfm?doid=1182475.1182491},
  urldate = {2022-08-21},
  eventtitle = {The 4th {{Nordic}} Conference},
  isbn = {978-1-59593-325-6},
  langid = {english}
}

@article{medlockUsingRITEMethod2007,
  title = {Using the {{RITE}} Method to Improve Products; a Definition and a Case Study},
  author = {Medlock, Michael C. and Wixon, Dennis and Terrano, Mark and Romero, Ramon L.},
  date = {2007-01-01},
  url = {https://www.scinapse.io/papers/48202622},
  urldate = {2023-03-29},
  abstract = {This paper defines and evaluates a method that some practitioners are using but has not been formally discusse},
  langid = {english}
}

@inproceedings{mirriAccessibleGraphsHTMLbased2017,
  title = {Towards Accessible Graphs in {{HTML-based}} Scientific Articles},
  booktitle = {2017 14th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  author = {Mirri, Silvia and Peroni, Silvio and Salomoni, Paola and Vitali, Fabio and Rubano, Vincenzo},
  date = {2017-01},
  pages = {1067--1072},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV}},
  doi = {10.1109/CCNC.2017.7983287},
  url = {https://ieeexplore.ieee.org/document/7983287/},
  urldate = {2022-08-21},
  eventtitle = {2017 14th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  isbn = {978-1-5090-6196-9}
}

@article{morashGuidingNoviceWeb2015,
  title = {Guiding {{Novice Web Workers}} in {{Making Image Descriptions Using Templates}}},
  author = {Morash, Valerie S. and Siu, Yue-Ting and Miele, Joshua A. and Hasty, Lucia and Landau, Steven},
  date = {2015-11-19},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume = {7},
  number = {4},
  pages = {12:1--12:21},
  issn = {1936-7228},
  doi = {10.1145/2764916},
  url = {https://dl.acm.org/doi/10.1145/2764916},
  urldate = {2023-09-10},
  abstract = {This article compares two methods of employing novice Web workers to author descriptions of science, technology, engineering, and mathematics images to make them accessible to individuals with visual and print-reading disabilities. The goal is to identify methods of creating image descriptions that are inexpensive, effective, and follow established accessibility guidelines. The first method explicitly presented the guidelines to the worker, then the worker constructed the image description in an empty text box and table. The second method queried the worker for image information and then used responses to construct a template-based description according to established guidelines. The descriptions generated through queried image description (QID) were more likely to include information on the image category, title, caption, and units. They were also more similar to one another, based on Jaccard distances of q-grams, indicating that their word usage and structure were more standardized. Last, the workers preferred describing images using QID and found the task easier. Therefore, explicit instruction on image-description guidelines is not sufficient to produce quality image descriptions when using novice Web workers. Instead, it is better to provide information about images, then generate descriptions from responses using templates.},
  keywords = {access technology,Accessibility (blind and visually impaired),crowdsourcing,human computation,image description},
  file = {C:\Users\jseo1005\Zotero\storage\RH4YUIIR\Morash et al. - 2015 - Guiding Novice Web Workers in Making Image Descrip.pdf}
}

@software{OlliScreenReader2022,
  title = {Olli - {{Screen Reader Accessibility}} for {{Data Visualization}}},
  date = {2022-10-05T01:58:04Z},
  origdate = {2022-05-10T18:27:58Z},
  url = {https://github.com/mitvis/olli},
  urldate = {2022-10-05},
  abstract = {A library for converting web visualizations into accessible text structures for blind and low-vision screen reader users.},
  organization = {{MIT Visualization Group}}
}

@article{omodhrainDesigningMediaVisuallyImpaired2015,
  title = {Designing {{Media}} for {{Visually-Impaired Users}} of {{Refreshable Touch Displays}}: {{Possibilities}} and {{Pitfalls}}},
  shorttitle = {Designing {{Media}} for {{Visually-Impaired Users}} of {{Refreshable Touch Displays}}},
  author = {O’Modhrain, Sile and Giudice, Nicholas A. and Gardner, John A. and Legge, Gordon E.},
  date = {2015-07},
  journaltitle = {IEEE Transactions on Haptics},
  volume = {8},
  number = {3},
  pages = {248--257},
  issn = {2329-4051},
  doi = {10.1109/TOH.2015.2466231},
  abstract = {This paper discusses issues of importance to designers of media for visually impaired users. The paper considers the influence of human factors on the effectiveness of presentation as well as the strengths and weaknesses of tactile, vibrotactile, haptic, and multimodal methods of rendering maps, graphs, and models. The authors, all of whom are visually impaired researchers in this domain, present findings from their own work and work of many others who have contributed to the current understanding of how to prepare and render images for both hard-copy and technology-mediated presentation of Braille and tangible graphics.},
  eventtitle = {{{IEEE Transactions}} on {{Haptics}}},
  keywords = {haptic display,Haptic interfaces,Media,refreshible braille.,Rendering (computer graphics),Shape,Solid modeling,tactile display,Tangible graphics,touch display,vibrotactile display,Visualization,visually impaired},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\4B8UTWDV\\O’Modhrain et al. - 2015 - Designing Media for Visually-Impaired Users of Ref.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\UPVMSZ6B\\7182782.html}
}

@online{organIncompleteGuideAccessible2021,
  title = {An {{Incomplete Guide}} to {{Accessible Data Visualization}}},
  author = {Organ, Nancy},
  date = {2021-08-05T16:43:46},
  url = {https://towardsdatascience.com/an-incomplete-guide-to-accessible-data-visualization-33f15bfcc400},
  urldate = {2022-08-21},
  abstract = {Practical tips for doing even a little bit better.},
  langid = {english},
  organization = {{Medium}},
  file = {C:\Users\jseo1005\Zotero\storage\9SC33Q59\an-incomplete-guide-to-accessible-data-visualization-33f15bfcc400.html}
}

@article{panditAgileUATFrameworkUser2015,
  title = {{{AgileUAT}}: {{A Framework}} for {{User Acceptance Testing}} Based on {{User Stories}} and {{Acceptance Criteria}}},
  shorttitle = {{{AgileUAT}}},
  author = {Pandit, Pallavi and Tahiliani, Swati},
  date = {2015-06-18},
  journaltitle = {International Journal of Computer Applications},
  shortjournal = {IJCA},
  volume = {120},
  number = {10},
  pages = {16--21},
  issn = {09758887},
  doi = {10.5120/21262-3533},
  url = {http://research.ijcaonline.org/volume120/number10/pxc3903533.pdf},
  urldate = {2023-03-13},
  abstract = {User Acceptance Testing (UAT) has widespread implications in the software community. It involves not only the end-user, but the Quality Assurance (QA) team, developers, business analysts and top level management. UAT is conducted with the aim of developing confidence of the user in the software product. UAT is generally performed manually and not preferred to be automated. UAT frameworks exist for Agile methodologies such as Scrum. We propose a UAT process model which adapts the generic agile process model. Hence, it is able to encompass every agile methodology. AgileUAT, aims at generation of exhaustive acceptance test cases in natural language, based on acceptance criteria. It indicates whether the acceptance criteria is fulfilled or not, as a percentage value. The tool illustrates traceability among epics, user stories, acceptance criteria and acceptance test cases. We explore several different templates for user stories and acceptance criteria. In the future, we aim to provide a direct mapping between the acceptance criteria and acceptance test cases based on permutations and combinations using decision tables.},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\CGHSKN8Y\Pandit and Tahiliani - 2015 - AgileUAT A Framework for User Acceptance Testing .pdf}
}

@article{paneelsReviewDesignsHaptic2010,
  title = {Review of {{Designs}} for {{Haptic Data Visualization}}},
  author = {Paneels, Sabrina and Roberts, Jonathan C.},
  date = {2010-04},
  journaltitle = {IEEE Transactions on Haptics},
  volume = {3},
  number = {2},
  pages = {119--137},
  issn = {2329-4051},
  doi = {10.1109/TOH.2009.44},
  abstract = {There are many different uses for haptics, such as training medical practitioners, teleoperation, or navigation of virtual environments. This review focuses on haptic methods that display data. The hypothesis is that haptic devices can be used to present information, and consequently, the user gains quantitative, qualitative, or holistic knowledge about the presented data. Not only is this useful for users who are blind or partially sighted (who can feel line graphs, for instance), but also the haptic modality can be used alongside other modalities, to increase the amount of variables being presented, or to duplicate some variables to reinforce the presentation. Over the last 20 years, a significant amount of research has been done in haptic data presentation; e.g., researchers have developed force feedback line graphs, bar charts, and other forms of haptic representations. However, previous research is published in different conferences and journals, with different application emphases. This paper gathers and collates these various designs to provide a comprehensive review of designs for haptic data visualization. The designs are classified by their representation: Charts, Maps, Signs, Networks, Diagrams, Images, and Tables. This review provides a comprehensive reference for researchers and learners, and highlights areas for further research.},
  eventtitle = {{{IEEE Transactions}} on {{Haptics}}},
  keywords = {Biomedical imaging,Data visualization,Displays,Force feedback,Frequency,Haptic data visualization,haptic design.,Haptic interfaces,haptic visualization,haptics,Mathematical model,Navigation,non-visual visualization,Rendering (computer graphics),Virtual environment},
  file = {C:\Users\jseo1005\Zotero\storage\XW9SILTL\Paneels and Roberts - 2010 - Review of Designs for Haptic Data Visualization.pdf}
}

@inproceedings{pengSayItAll2021,
  title = {Say {{It All}}: {{Feedback}} for {{Improving Non-Visual Presentation Accessibility}}},
  shorttitle = {Say {{It All}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Peng, Yi-Hao and Jang, JiWoong and Bigham, Jeffrey P and Pavel, Amy},
  date = {2021-05-06},
  pages = {1--12},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445572},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445572},
  urldate = {2022-12-07},
  abstract = {Presenters commonly use slides as visual aids for informative talks. When presenters fail to verbally describe the content on their slides, blind and visually impaired audience members lose access to necessary content, making the presentation difcult to follow. Our analysis of 90 presentation videos revealed that 72\% of 610 visual elements (e.g., images, text) were insufciently described. To help presenters create accessible presentations, we introduce Presentation A11y, a system that provides real-time and post-presentation accessibility feedback. Our system analyzes visual elements on the slide and the transcript of the verbal presentation to provide element-level feedback on what visual content needs to be further described or even removed. Presenters using our system with their own slide-based presentations described more of the content on their slides, and identifed 3.26 times more accessibility problems to fx after the talk than when using a traditional slide-based presentation interface. Integrating accessibility feedback into content creation tools will improve the accessibility of informational content for all.},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\LI5QXB65\Peng et al. - 2021 - Say It All Feedback for Improving Non-Visual Pres.pdf}
}

@inproceedings{potluriCodeTalkImprovingProgramming2018,
  title = {{{CodeTalk}}: {{Improving Programming Environment Accessibility}} for {{Visually Impaired Developers}}},
  shorttitle = {{{CodeTalk}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Potluri, Venkatesh and Vaithilingam, Priyan and Iyengar, Suresh and Vidya, Y. and Swaminathan, Manohar and Srinivasa, Gopal},
  date = {2018-04-21},
  series = {{{CHI}} '18},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3173574.3174192},
  url = {https://doi.org/10.1145/3173574.3174192},
  urldate = {2022-12-09},
  abstract = {In recent times, programming environments like Visual Studio are widely used to enhance programmer productivity. However, inadequate accessibility prevents Visually Impaired (VI) developers from taking full advantage of these environments. In this paper, we focus on the accessibility challenges faced by the VI developers in using Graphical User Interface (GUI) based programming environments. Based on a survey of VI developers and based on two of the authors' personal experiences, we categorize the accessibility difficulties into Discoverability, Glanceability, Navigability, and Alertability. We propose solutions to some of these challenges and implement these in CodeTalk, a plugin for Visual Studio. We show how CodeTalk improves developer experience and share promising early feedback from VI developers who used our plugin.},
  isbn = {978-1-4503-5620-6},
  keywords = {accessibility,audio debugging,programming environments,visually impaired},
  file = {C:\Users\jseo1005\Zotero\storage\6D4HNELI\Potluri et al. - 2018 - CodeTalk Improving Programming Environment Access.pdf}
}

@inproceedings{potluriNotablyInaccessibleData2023,
  title = {Notably {{Inaccessible}} — {{Data Driven Understanding}} of {{Data Science Notebook}} ({{In}}){{Accessibility}}},
  booktitle = {Proceedings of the 25th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Potluri, Venkatesh and Singanamalla, Sudheesh and Tieanklin, Nussara and Mankoff, Jennifer},
  date = {2023-10-22},
  series = {{{ASSETS}} '23},
  pages = {1--19},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3597638.3608417},
  url = {https://dl.acm.org/doi/10.1145/3597638.3608417},
  urldate = {2023-11-01},
  abstract = {Computational notebooks, tools that facilitate storytelling through exploration, data analysis, and information visualization, have become the widely accepted standard in the data science community. These notebooks have been widely adopted through notebook software such as Jupyter, Datalore and Google Colab, both in academia and industry. While there is extensive research to learn how data scientists use computational notebooks, identify their pain points, and enable collaborative data science practices, very little is known about the various accessibility barriers experienced by blind and visually impaired (BVI) users using these notebooks. BVI users are unable to use computational notebook interfaces due to (1) inaccessibility of the interface, (2) common ways in which data is represented in these interfaces, and (3) inability for popular libraries to provide accessible outputs. We perform a large scale systematic analysis of 100000 Jupyter notebooks to identify various accessibility challenges in published notebooks affecting the creation and consumption of these notebooks. Through our findings, we make recommendations to improve accessibility of the artifacts of a notebook, suggest authoring practices, and propose changes to infrastructure to make notebooks accessible.},
  isbn = {9798400702204},
  keywords = {Accessibility,computational notebooks,Data science,measurement},
  file = {C:\Users\jseo1005\Zotero\storage\I5ZLZCY2\Potluri et al. - 2023 - Notably Inaccessible — Data Driven Understanding o.pdf}
}

@inproceedings{rochaSpeechtoTextInterfaceMammoClass2016,
  title = {A {{Speech-to-Text Interface}} for {{MammoClass}}},
  booktitle = {2016 {{IEEE}} 29th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Rocha, Ricardo Sousa and Ferreira, Pedro and Dutra, Ines and Correia, Ricardo and Salvini, Rogerio and Burnside, Elizabeth},
  date = {2016-06},
  pages = {1--6},
  publisher = {{IEEE}},
  location = {{Belfast and Dublin, Ireland}},
  doi = {10.1109/CBMS.2016.25},
  url = {http://ieeexplore.ieee.org/document/7545947/},
  urldate = {2022-08-21},
  eventtitle = {2016 {{IEEE}} 29th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  isbn = {978-1-4673-9036-1},
  file = {C:\Users\jseo1005\Zotero\storage\F3Q2HP3D\Rocha et al. - 2016 - A Speech-to-Text Interface for MammoClass.pdf}
}

@inproceedings{salberApplyingWizardOz1993,
  title = {Applying the {{Wizard}} of {{Oz}} Technique to the Study of Multimodal Systems},
  booktitle = {Human-{{Computer Interaction}}},
  author = {Salber, Daniel and Coutaz, Joëlle},
  editor = {Bass, Leonard J. and Gornostaev, Juri and Unger, Claus},
  date = {1993},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {219--230},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-57433-6_51},
  abstract = {The Wizard of Oz (WOz) technique is an experimental evaluation mechanism. It allows the observation of a user operating an apparently fully functioning system whose missing services are supplemented by a hidden wizard. From our analysis of existing WOz systems, we observe that this technique has primarily been used to study natural language interfaces. With recent advances in interactive media, multimodal user interfaces are becoming popular but our current understanding on how to design such systems is still primitive. In the absence of generalizable theories and models, the WOz technique is an appropriate approach to the identification of sound design solutions. We show how the WOz technique can be extended to the analysis of multimodal interfaces and we formulate a set of requirements for a generic multimodal WOz platform. The Neimo system is presented as an illustration of our early experience in the development of such platforms.},
  isbn = {978-3-540-48152-2},
  langid = {english},
  keywords = {Client Application,Evaluation Expert,Multimodal Interaction,Multimodal Interface,Multimodal System},
  file = {C:\Users\jseo1005\Zotero\storage\28FEDWJI\Salber and Coutaz - 1993 - Applying the Wizard of Oz technique to the study o.pdf}
}

@article{sandersCocreationNewLandscapes2008,
  title = {Co-Creation and the New Landscapes of Design},
  author = {Sanders, Elizabeth B.-N. and Stappers, Pieter Jan},
  date = {2008-03-01},
  journaltitle = {CoDesign},
  volume = {4},
  number = {1},
  pages = {5--18},
  publisher = {{Taylor \& Francis}},
  issn = {1571-0882},
  doi = {10.1080/15710880701875068},
  url = {https://doi.org/10.1080/15710880701875068},
  urldate = {2023-09-13},
  abstract = {Designers have been moving increasingly closer to the future users of what they design and the next new thing in the changing landscape of design research has become co-designing with your users. But co-designing is actually not new at all, having taken distinctly different paths in the US and in Europe. The evolution in design research from a user-centred approach to co-designing is changing the roles of the designer, the researcher and the person formerly known as the ‘user’. The implications of this shift for the education of designers and researchers are enormous. The evolution in design research from a user-centred approach to co-designing is changing the landscape of design practice as well, creating new domains of collective creativity. It is hoped that this evolution will support a transformation toward more sustainable ways of living in the future.},
  keywords = {co-creation,co-design,collective creativity,design research,participatory design,user-centred design},
  file = {C:\Users\jseo1005\Zotero\storage\M3ZKHHRP\Sanders and Stappers - 2008 - Co-creation and the new landscapes of design.pdf}
}

@online{SASHelpCenter,
  title = {{{SAS Help Center}}: {{About}} the {{SAS Graphics Accelerator}}},
  url = {https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/odsacoutput/p1363trzxif9zun1rq43uo5ys7q3.htm},
  urldate = {2022-08-20},
  file = {C:\Users\jseo1005\Zotero\storage\8GKLQ229\p1363trzxif9zun1rq43uo5ys7q3.html}
}

@online{schepersWhyAccessibilityHeart2020,
  title = {Why {{Accessibility Is}} at the {{Heart}} of {{Data Visualization}}},
  author = {Schepers, Doug},
  date = {2020-05-21T17:12:50},
  url = {https://medium.com/nightingale/accessibility-is-at-the-heart-of-data-visualization-64a38d6c505b},
  urldate = {2023-02-06},
  abstract = {To make data viz more accessible, we first need to understand assistive technology},
  langid = {english},
  organization = {{Nightingale}}
}

@inproceedings{seoMAIDRMultimodalAccess2023,
  title = {{{MAIDR}}: {{Multimodal Access}} and {{Interactive Data Representation System}} for {{Inclusive Data Science Education}}},
  booktitle = {Proceedings of the 3rd {{Annual Meeting}} of the {{International Society}} of the {{Learning Sciences}}},
  author = {Seo, JooYoung and Xia, Yilin and Yam, Yu Jun and McCurry, Sean},
  date = {2023},
  pages = {51--54}
}

@article{seoTeachingVisualAccessibility2023,
  title = {Teaching {{Visual Accessibility}} in {{Introductory Data Science Classes}} with {{Multi-Modal Data Representations}}},
  author = {Seo, JooYoung and Dogucu, Mine},
  date = {2023-03-21},
  journaltitle = {Journal of Data Science},
  pages = {1--14},
  publisher = {{School of Statistics, Renmin University of China}},
  issn = {1680-743X, 1683-8602},
  doi = {10.6339/23-JDS1095},
  url = {https://jds-online.org/journal/JDS/article/1331},
  urldate = {2023-04-10},
  abstract = {Although there are various ways to represent data patterns and models, visualization has been primarily taught in many data science courses for its efficiency. Such vision-dependent output may cause critical barriers against those who are blind and visually impaired and people with learning disabilities. We argue that instructors need to teach multiple data representation methods so that all students can produce data products that are more accessible. In this paper, we argue that accessibility should be taught as early as the introductory course as part of the data science curriculum so that regardless of whether learners major in data science or not, they can have foundational exposure to accessibility. As data science educators who teach accessibility as part of our lower-division courses in two different institutions, we share specific examples that can be utilized by other data science instructors.},
  langid = {english}
}

@article{seoTeachingVisualAccessibilityinpress,
  title = {Teaching {{Visual Accessibility}} in {{Introductory Data Science Classes}} with {{Multi-Modal Data Representations}}},
  author = {Seo, JooYoung and Dogucu, Mine},
  year = {in press},
  journaltitle = {Journal of Data Science},
  shortjournal = {JDS},
  eprint = {2208.02565},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2208.02565},
  urldate = {2023-03-11},
  abstract = {Although there are various ways to represent data patterns and models, visualization has been primarily taught in many data science courses for its efficiency. Such vision-dependent output may cause critical barriers against those who are blind and visually impaired and people with learning disabilities. We argue that instructors need to teach multiple data representation methods so that all students can produce data products that are more accessible. In this paper, we argue that accessibility should be taught as early as the introductory course as part of the data science curriculum so that regardless of whether learners major in data science or not, they can have foundational exposure to accessibility. As data science educators who teach accessibility as part of our lower-division courses in two different institutions, we share specific examples that can be utilized by other data science instructors.},
  keywords = {Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Statistics - Other Statistics},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\PD39TUQC\\Seo and Dogucu - 2022 - Teaching Visual Accessibility in Introductory Data.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\IYUE66BQ\\2208.html}
}

@inproceedings{sharifEvoGraphsJQueryPlugin2018,
  title = {{{evoGraphs}} — {{A jQuery}} Plugin to Create Web Accessible Graphs},
  booktitle = {2018 15th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  author = {Sharif, Ather and Forouraghi, Babak},
  date = {2018-01},
  pages = {1--4},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV}},
  doi = {10.1109/CCNC.2018.8319239},
  url = {http://ieeexplore.ieee.org/document/8319239/},
  urldate = {2022-08-21},
  eventtitle = {2018 15th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  isbn = {978-1-5386-4790-5}
}

@inproceedings{sharifUnderstandingScreenReaderUsers2021,
  title = {Understanding {{Screen-Reader Users}}’ {{Experiences}} with {{Online Data Visualizations}}},
  booktitle = {The 23rd {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Sharif, Ather and Chintalapati, Sanjana Shivani and Wobbrock, Jacob O. and Reinecke, Katharina},
  date = {2021-10-17},
  series = {{{ASSETS}} '21},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3441852.3471202},
  url = {https://doi.org/10.1145/3441852.3471202},
  urldate = {2022-08-21},
  abstract = {Online data visualizations are widely used to communicate information from simple statistics to complex phenomena, supporting people in gaining important insights from data. However, due to the defining visual nature of data visualizations, extracting information from visualizations can be difficult or impossible for screen-reader users. To assess screen-reader users’ challenges with online data visualizations, we conducted two empirical studies: (1) A qualitative study with nine screen-reader users, and (2) a quantitative study with 36 screen-reader and 36 non-screen-reader users. Our results show that due to the inaccessibility of online data visualizations, screen-reader users extract information 61.48\% less accurately and spend 210.96\% more time interacting with online data visualizations compared to non-screen-reader users. Additionally, our findings show that online data visualizations are commonly indiscoverable to screen readers. In visualizations that are discoverable and comprehensible, screen-reader users suggested tabular and textual representation of data as techniques to improve the accessibility of online visualizations. Taken together, our results provide empirical evidence of the inequalities screen-readers users face in their interaction with online data visualizations.},
  isbn = {978-1-4503-8306-6},
  keywords = {challenges,data,screen readers,techniques,visualizations}
}

@software{sharifVoxLens2022,
  title = {{{VoxLens}}},
  author = {Sharif, Ather},
  date = {2022-07-11T20:47:53Z},
  origdate = {2021-10-06T21:56:25Z},
  url = {https://github.com/athersharif/voxlens},
  urldate = {2022-08-21},
  abstract = {JavaScript Library to Make Online Data Visualizations Accessible to Screen-Reader Users},
  keywords = {accessibility,blind,dataviz,graph,javascript,visualization}
}

@inproceedings{sharifVoxLensMakingOnline2022,
  title = {{{VoxLens}}: {{Making Online Data Visualizations Accessible}} with an {{Interactive JavaScript Plug-In}}},
  shorttitle = {{{VoxLens}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sharif, Ather and Wang, Olivia H. and Muongchan, Alida T. and Reinecke, Katharina and Wobbrock, Jacob O.},
  date = {2022-04-29},
  pages = {1--19},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3517431},
  url = {https://dl.acm.org/doi/10.1145/3491102.3517431},
  urldate = {2022-08-21},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\L9SM5CPR\Sharif et al. - 2022 - VoxLens Making Online Data Visualizations Accessi.pdf}
}

@inproceedings{siuCOVID19HighlightsIssues2021,
  title = {{{COVID-19}} Highlights the Issues Facing Blind and Visually Impaired People in Accessing Data on the Web},
  booktitle = {Proceedings of the 18th {{International Web}} for {{All Conference}}},
  author = {Siu, Alexa F. and Fan, Danyang and Kim, Gene S-H and Rao, Hrishikesh V. and Vazquez, Xavier and O'Modhrain, Sile and Follmer, Sean},
  date = {2021-04-19},
  pages = {1--15},
  publisher = {{ACM}},
  location = {{Ljubljana Slovenia}},
  doi = {10.1145/3430263.3452432},
  url = {https://dl.acm.org/doi/10.1145/3430263.3452432},
  urldate = {2023-01-12},
  abstract = {During the COVID-19 pandemic, dissemination of data on the web has been vital in shaping the public’s response. We postulated the increased prominence of data might have exacerbated the accessibility gap for the Blind and Visually Impaired (BVI) community and exposed new inequities. We discuss fndings from a survey (n=127) on data accessibility followed by a contextual inquiry (n=12) of BVI people conducted between June and September 2020. 94\% of survey respondents had concerns about accessing accurate COVID-19 data in a timely manner. Participants described how they encountered broad inaccessibility at early onset of the pandemic, and how advocacy eforts and complimenting their access with a wide range of sources helped fulfll their needs. By examining how BVI users interact with accessible COVID-19 data dashboards, we observed the efect of data literacy, confdence, and modality preferences on user strategies and takeaways. Our observations during this critical period provide an understanding of the impact access or inaccess has on the BVI community in a time of crisis and important implications for improving the technologies and modalities available to disseminate data-driven information accessibly on the web.},
  eventtitle = {{{W4A}} '21: 18th {{Web}} for {{All Conference}}},
  isbn = {978-1-4503-8212-0},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\6UTD4QYV\Siu et al. - 2021 - COVID-19 highlights the issues facing blind and vi.pdf}
}

@inproceedings{siuSupportingAccessibleData2022,
  title = {Supporting {{Accessible Data Visualization Through Audio Data Narratives}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Siu, Alexa and S-H Kim, Gene and O'Modhrain, Sile and Follmer, Sean},
  date = {2022-04-29},
  pages = {1--19},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3517678},
  url = {https://dl.acm.org/doi/10.1145/3491102.3517678},
  urldate = {2023-01-12},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\KG9VB5WG\Siu et al. - 2022 - Supporting Accessible Data Visualization Through A.pdf}
}

@inproceedings{siuSupportingAccessibleData2022b,
  title = {Supporting {{Accessible Data Visualization Through Audio Data Narratives}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Siu, Alexa and S-H Kim, Gene and O'Modhrain, Sile and Follmer, Sean},
  date = {2022-04-29},
  pages = {1--19},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3517678},
  url = {https://dl.acm.org/doi/10.1145/3491102.3517678},
  urldate = {2023-09-05},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\I2LXSHBV\Siu et al. - 2022 - Supporting Accessible Data Visualization Through A.pdf}
}

@online{SonificationHandbookEdited,
  title = {The {{Sonification Handbook}} | Edited by {{Hermann}}, {{Hunt}}, {{Neuhoff}}},
  url = {https://sonification.de/handbook/},
  urldate = {2022-09-20},
  langid = {american},
  file = {C:\Users\jseo1005\Zotero\storage\4T78LXUJ\handbook.html}
}

@inproceedings{srinivasanAzimuthDesigningAccessible2023,
  title = {Azimuth: {{Designing Accessible Dashboards}} for {{Screen Reader Users}}},
  shorttitle = {Azimuth},
  booktitle = {Proceedings of the 25th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Srinivasan, Arjun and Harshbarger, Tim and Hilliker, Darrell and Mankoff, Jennifer},
  date = {2023-10-22},
  series = {{{ASSETS}} '23},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3597638.3608405},
  url = {https://dl.acm.org/doi/10.1145/3597638.3608405},
  urldate = {2023-11-01},
  abstract = {Dashboards are frequently used to monitor and share data across a breadth of domains including business, finance, sports, public policy, and healthcare, just to name a few. The combination of different components (e.g., key performance indicators, charts, filtering widgets) and the interactivity between components makes dashboards powerful interfaces for data monitoring and analysis. However, these very characteristics also often make dashboards inaccessible to blind and low vision (BLV) users. Through a co-design study with two screen reader users, we investigate challenges faced by BLV users and identify design goals to support effective screen reader-based interactions with dashboards. Operationalizing the findings from the co-design process, we present a prototype system, Azimuth, that generates dashboards optimized for screen reader-based navigation along with complementary descriptions to support dashboard comprehension and interaction. Based on a follow-up study with five BLV participants, we showcase how our generated dashboards support BLV users and enable them to perform both targeted and open-ended analysis. Reflecting on our design process and study feedback, we discuss opportunities for future work on supporting interactive data analysis, understanding dashboard accessibility at scale, and investigating alternative devices and modalities for designing accessible visualization dashboards.},
  isbn = {9798400702204},
  keywords = {Dashboards,screen readers,text generation},
  file = {C:\Users\jseo1005\Zotero\storage\QWMZIF9G\Srinivasan et al. - 2023 - Azimuth Designing Accessible Dashboards for Scree.pdf}
}

@inproceedings{srinivasanCollectingCharacterizingNatural2021,
  title = {Collecting and {{Characterizing Natural Language Utterances}} for {{Specifying Data Visualizations}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Srinivasan, Arjun and Nyapathy, Nikhila and Lee, Bongshin and Drucker, Steven M. and Stasko, John},
  date = {2021-05-06},
  pages = {1--10},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445400},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445400},
  urldate = {2022-08-21},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\BLM3Z6L4\Srinivasan et al. - 2021 - Collecting and Characterizing Natural Language Utt.pdf}
}

@online{StateDiagramVisualizing,
  title = {State {{Diagram}}: {{Visualizing}} the {{Behavior}} and {{Transitions}} of {{Systems}}},
  shorttitle = {State {{Diagram}}},
  url = {https://start-up.house/inventory/state-diagram},
  urldate = {2023-09-07},
  abstract = {State diagram: a visual representation of system behaviors and transitions.},
  langid = {english},
  organization = {{Startup House}}
}

@inproceedings{suhDevelopingValidatingUser2016,
  title = {Developing and {{Validating}} the {{User Burden Scale}}: {{A Tool}} for {{Assessing User Burden}} in {{Computing Systems}}},
  shorttitle = {Developing and {{Validating}} the {{User Burden Scale}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Suh, Hyewon and Shahriaree, Nina and Hekler, Eric B. and Kientz, Julie A.},
  date = {2016-05-07},
  series = {{{CHI}} '16},
  pages = {3988--3999},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2858036.2858448},
  url = {https://doi.org/10.1145/2858036.2858448},
  urldate = {2022-12-16},
  abstract = {Computing systems that place a high level of burden on their users can have a negative affect on initial adoption, retention, and overall user experience. Through an iterative process, we have developed a model for user burden that consists of six constructs: 1) difficulty of use, 2) physical, 3) time and social, 4) mental and emotional, 5) privacy, and 6) financial. If researchers and practitioners can have an understanding of the overall level of burden systems may be having on the user, they can have a better sense of whether and where to target future design efforts that can reduce those burdens. To help assist with understanding and measuring user burden, we have also developed and validated a measure of user burden in computing systems called the User Burden Scale (UBS), which is a 20-item scale with 6 individual sub-scales representing each construct. This paper presents the process we followed to develop and validate this scale for use in evaluating user burden in computing systems. Results indicate that the User Burden Scale has good overall inter-item reliability, convergent validity with similar scales, and concurrent validity when compared to systems abandoned vs. those still in use.},
  isbn = {978-1-4503-3362-7},
  keywords = {evaluation,measuring usability,technology abandonment,usability,user burden,user experience,validated measures},
  file = {C:\Users\jseo1005\Zotero\storage\X4KP6UF5\Suh et al. - 2016 - Developing and Validating the User Burden Scale A.pdf}
}

@article{summersAccessibilityODSGraphics,
  title = {Accessibility and {{ODS Graphics}}: {{Seven Simple Steps}} to {{Section}} 508 {{Compliance Using SAS}}® 9.{{4M5}}},
  author = {Summers, Ed and Langston, Julianna and Heath, Dan},
  pages = {13},
  abstract = {How do you create data visualizations that comply with the Section 508 amendment to the United States Workforce Rehabilitation Act, the Web Content Accessibility Guidelines (WCAG), and other accessibility standards? It’s easy when you use the new Output Delivery System (ODS) Graphics accessibility features in SAS® 9.4M5. This paper defines seven simple steps to create accessible data visualizations. The accessibility requirements that are satisfied by each step are explained, and additional references are provided. It includes sample code for real-world examples that has been tested by the SAS® accessibility team. It also includes a handy one-page checklist that you can print separately for future reference.},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\9X2W8VD8\Summers et al. - Accessibility and ODS Graphics Seven Simple Steps.pdf}
}

@patent{summersAccessibleDataVisualizations2014,
  type = {patentus},
  title = {Accessible {{Data Visualizations}} for {{Visually Impaired Users}}},
  author = {Summers, II Claude Edward and JR, Robert E. Allison and Langston, Julianna Elizabeth and Cowley, Jennifer Antonia},
  holder = {{SAS Institute Inc}},
  date = {2014-02-20},
  number = {20140053055A1},
  url = {https://patents.google.com/patent/US20140053055/en},
  urldate = {2022-08-19},
  langid = {english},
  keywords = {accessibility information,boundary,graphic,region,visualization},
  file = {C:\Users\jseo1005\Zotero\storage\UGHK8CJK\Summers et al. - 2014 - Accessible Data Visualizations for Visually Impair.pdf}
}

@patent{summersConvertingGraphicalDatavisualizations2019,
  type = {patentus},
  title = {Converting Graphical Data-Visualizations into Sonified Output},
  author = {Summers, II Claude Edward and Langston, Julianna Elizabeth and Sookne, Jesse Daniel and Olley, Jesse Benjamin and Trout, Kerry Leanne Smith and IV, Cleester Daniel Heath and Kalat, Samuel Edward and Layne, Paul William},
  holder = {{SAS Institute Inc}},
  date = {2019-01-29},
  number = {10191979B2},
  url = {https://patents.google.com/patent/US10191979B2/en},
  urldate = {2022-08-19},
  langid = {english},
  keywords = {data,data point,output,processing device,sonified},
  file = {C:\Users\jseo1005\Zotero\storage\MGDD7QSW\Summers et al. - 2019 - Converting graphical data-visualizations into soni.pdf}
}

@patent{summersUserInterfacesConverting2022,
  type = {patentus},
  title = {User Interfaces for Converting Geospatial Data into Audio Outputs},
  author = {Summers, II Claude Edward and Mealin, Sean Patrick and Langston, Julianna Elizabeth and Kraus, Gregory David and Williamson, Jonathan Tyler and Robinson, Lisa Beth Morton and Sookne, Jesse Daniel and Smith, Brice Joseph},
  holder = {{SAS Institute Inc}},
  date = {2022-02-22},
  number = {11257396B2},
  url = {https://patents.google.com/patent/US11257396B2/en/},
  urldate = {2022-09-21},
  langid = {english},
  keywords = {data,map,user,virtual,virtual map},
  file = {C:\Users\jseo1005\Zotero\storage\5LC4C6VM\Summers et al. - 2022 - User interfaces for converting geospatial data int.pdf}
}

@book{SUSQuickDirty1996,
  title = {{{SUS}}: {{A}} '{{Quick}} and {{Dirty}}' {{Usability Scale}}},
  shorttitle = {{{SUS}}},
  date = {1996-06-11},
  journaltitle = {Usability Evaluation In Industry},
  pages = {207--212},
  publisher = {{CRC Press}},
  doi = {10.1201/9781498710411-35},
  url = {https://www.taylorfrancis.com/chapters/edit/10.1201/9781498710411-35/sus-quick-dirty-usability-scale-john-brooke},
  urldate = {2022-12-16},
  abstract = {Usability is not a quality that exists in any real or absolute sense. Perhaps it can be best summed up as being a general quality of the appropriateness to a purpose of any particular artefact. This notion is neatly summed up by Terry Pratchett in his novel Moving Pictures:In just the same way, the usability of any tool or system has to be viewed in terms of the context in which it is used, and its appropriateness to that context. With particular reference to information systems, this view of usability is reflected in the current draft international standard ISO 9241-11 and in the European Community ESPRIT project MUSiC (Measuring Usability of Systems in Context) (e.g. Bevan et al., 1991). In general, it is impossible to specify the usability of a system (i.e. its fitness for purpose) without first defining who are the intended users of the system, the tasks those users will perform with it, and the characteristics of the physical, organizational and social environment in which it will be used.},
  isbn = {978-0-429-15701-1},
  langid = {english}
}

@article{swellerCognitiveLoadProblem1988,
  title = {Cognitive Load during Problem Solving: {{Effects}} on Learning},
  shorttitle = {Cognitive Load during Problem Solving},
  author = {Sweller, John},
  date = {1988-04-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {12},
  number = {2},
  pages = {257--285},
  issn = {0364-0213},
  doi = {10.1016/0364-0213(88)90023-7},
  url = {https://www.sciencedirect.com/science/article/pii/0364021388900237},
  urldate = {2023-01-17},
  abstract = {Considerable evidence indicates that domain specific knowledge in the form of schemas is the primary factor distinguishing experts from novices in problem-solving skill. Evidence that conventional problem-solving activity is not effective in schema acquisition is also accumulating. It is suggested that a major reason for the ineffectiveness of problem solving as a learning device, is that the cognitive processes required by the two activities overlap insufficiently, and that conventional problem solving in the form of means-ends analysis requires a relatively large amount of cognitive processing capacity which is consequently unavailable for schema acquisition. A computational model and experimental evidence provide support for this contention. Theoretical and practical implications are discussed.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\EKMKBBLY\\Sweller - 1988 - Cognitive load during problem solving Effects on .pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\WMBAQZBS\\0364021388900237.html}
}

@article{swellerElementInteractivityIntrinsic2010,
  title = {Element {{Interactivity}} and {{Intrinsic}}, {{Extraneous}}, and {{Germane Cognitive Load}}},
  author = {Sweller, John},
  date = {2010-06-01},
  journaltitle = {Educational Psychology Review},
  shortjournal = {Educ Psychol Rev},
  volume = {22},
  number = {2},
  pages = {123--138},
  issn = {1573-336X},
  doi = {10.1007/s10648-010-9128-5},
  url = {https://doi.org/10.1007/s10648-010-9128-5},
  urldate = {2023-01-18},
  abstract = {In cognitive load theory, element interactivity has been used as the basic, defining mechanism of intrinsic cognitive load for many years. In this article, it is suggested that element interactivity underlies extraneous cognitive load as well. By defining extraneous cognitive load in terms of element interactivity, a distinct relation between intrinsic and extraneous cognitive load can be established based on whether element interactivity is essential to the task at hand or whether it is a function of instructional procedures. Furthermore, germane cognitive load can be defined in terms of intrinsic cognitive load, thus also associating germane cognitive load with element interactivity. An analysis of the consequences of explaining the various cognitive load effects in terms of element interactivity is carried out.},
  langid = {english},
  keywords = {Cognitive load theory,Element interactivity,Extraneous cognitive load,Germane cognitive load,Intrinsic cognitive load},
  file = {C:\Users\jseo1005\Zotero\storage\CBDT7FY5\Sweller - 2010 - Element Interactivity and Intrinsic, Extraneous, a.pdf}
}

@inproceedings{thompsonChartReaderAccessible2023,
  title = {Chart {{Reader}}: {{Accessible Visualization Experiences Designed}} with {{Screen Reader Users}}},
  shorttitle = {Chart {{Reader}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Thompson, John R and Martinez, Jesse J and Sarikaya, Alper and Cutrell, Edward and Lee, Bongshin},
  date = {2023-04-19},
  series = {{{CHI}} '23},
  pages = {1--18},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581186},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581186},
  urldate = {2023-04-25},
  abstract = {Even though screen readers are a core accessibility tool for blind and low vision individuals (BLVIs), most visualizations are incompatible with screen readers. To improve accessible visualization experiences, we partnered with 10 BLV screen reader users (SRUs) in an iterative co-design study to design and develop accessible visualization experiences that afford SRUs the autonomy to interactively read and understand visualizations and their underlying data. During the five-month study, we explored accessible visualization prototypes with our design partners for three one-hour sessions. Our results provide feedback on the synthesized design concepts we explored, why (or why not) they aid comprehension and exploration for SRUs, and how differing design concepts can fit into cohesive accessible visualization experiences. We contribute both Chart Reader, a web-based accessibility engine resulting from our design iterations, and our distilled study findings—organized by design dimensions—in the creation of comprehensive accessible visualization experiences.},
  isbn = {978-1-4503-9421-5},
  keywords = {accessibility,accessibility engine,accessible visualization experiences,blind and low vision,data visualization,iterative co-design,screen readers},
  file = {C:\Users\jseo1005\Zotero\storage\DHS55RVS\Thompson et al. - 2023 - Chart Reader Accessible Visualization Experiences.pdf}
}

@article{UnicodeStandardVersion,
  title = {The {{Unicode Standard}}, {{Version}} 15.0},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\3CGNFLBR\The Unicode Standard, Version 15.0.pdf}
}

@article{vinesSonificationNumericalData2019,
  title = {Sonification of Numerical Data for Education},
  author = {Vines, Karen and Hughes, Chris and Alexander, Laura and Calvert, Carol and Colwell, Chetz and Holmes, Hilary and Kotecki, Claire and Parks, Kaela and Pearson, Victoria},
  date = {2019-01-13},
  journaltitle = {Open Learning: The Journal of Open, Distance and e-Learning},
  publisher = {{Routledge}},
  issn = {0268-0513},
  url = {https://www.tandfonline.com/doi/full/10.1080/02680513.2018.1553707},
  urldate = {2022-12-16},
  abstract = {Sonification is the use of non-speech audio to convey information. In this article, sonifications are representations of plots aimed at improving the accessibility of teaching materials. The electr...},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\GQVU6QIM\02680513.2018.html}
}

@software{VisaChartComponents2022,
  title = {Visa {{Chart Components}}},
  date = {2022-10-15T03:25:41Z},
  origdate = {2020-11-11T01:29:46Z},
  url = {https://github.com/visa/visa-chart-components},
  urldate = {2022-10-20},
  abstract = {Visa Chart Components (VCC) is an accessibility focused, framework agnostic set of data experience design systems components for the web. VCC attempts to provide a toolset to enable developers to build equal data experiences for everyone, everywhere.},
  organization = {{Visa}},
  keywords = {accessibility,charts,d3,data-experience,data-visualization,graphs,stencil,visualization}
}

@online{VisualizationBlindACMInteractions,
  title = {Visualization for the {{BlindACM Interactions}}},
  url = {https://interactions.acm.org/archive/view/january-february-2023/visualization-for-the-blind},
  urldate = {2023-01-15},
  file = {C:\Users\jseo1005\Zotero\storage\BH8LR49A\visualization-for-the-blind.html}
}

@article{walkerMappingsMetaphorsAuditory2005,
  title = {Mappings and Metaphors in Auditory Displays: {{An}} Experimental Assessment},
  shorttitle = {Mappings and Metaphors in Auditory Displays},
  author = {Walker, Bruce N. and Kramer, Gregory},
  date = {2005-10-01},
  journaltitle = {ACM Transactions on Applied Perception},
  shortjournal = {ACM Trans. Appl. Percept.},
  volume = {2},
  number = {4},
  pages = {407--412},
  issn = {1544-3558},
  doi = {10.1145/1101530.1101534},
  url = {https://doi.org/10.1145/1101530.1101534},
  urldate = {2022-10-17},
  abstract = {Auditory displays are becoming more and more common, but there are still no general guidelines for mapping data dimensions (e.g., temperature) onto display dimensions (e.g., pitch). This paper presents experimental research on different mappings and metaphors, in a generic process-control task environment, with reaction time and accuracy as dependent measures. It is hoped that this area of investigation will lead to the development of mapping guidelines applicable to auditory displays in a wide range of task domains.},
  keywords = {auditory display,data mapping,guidelines,metaphors,Sonification}
}

@article{walkerSpearconsSpeechbasedEarcons2013,
  title = {Spearcons (Speech-Based Earcons) Improve Navigation Performance in Advanced Auditory Menus},
  author = {Walker, Bruce N. and Lindsay, Jeffrey and Nance, Amanda and Nakano, Yoko and Palladino, Dianne K. and Dingler, Tilman and Jeon, Myounghoon},
  date = {2013-02},
  journaltitle = {Human Factors},
  shortjournal = {Hum Factors},
  volume = {55},
  number = {1},
  eprint = {23516800},
  eprinttype = {pmid},
  pages = {157--182},
  issn = {0018-7208},
  doi = {10.1177/0018720812450587},
  abstract = {OBJECTIVE: The goal of this project is to evaluate a new auditory cue, which the authors call spearcons, in comparison to other auditory cues with the aim of improving auditory menu navigation. BACKGROUND: With the shrinking displays of mobile devices and increasing technology use by visually impaired users, it becomes important to improve usability of non-graphical user interface (GUI) interfaces such as auditory menus. Using nonspeech sounds called auditory icons (i.e., representative real sounds of objects or events) or earcons (i.e., brief musical melody patterns) has been proposed to enhance menu navigation. To compensate for the weaknesses of traditional nonspeech auditory cues, the authors developed spearcons by speeding up a spoken phrase, even to the point where it is no longer recognized as speech. METHOD: The authors conducted five empirical experiments. In Experiments 1 and 2, they measured menu navigation efficiency and accuracy among cues. In Experiments 3 and 4, they evaluated learning rate of cues and speech itself. In Experiment 5, they assessed spearcon enhancements compared to plain TTS (text to speech: speak out written menu items) in a two-dimensional auditory menu. RESULTS: Spearcons outperformed traditional and newer hybrid auditory cues in navigation efficiency, accuracy, and learning rate. Moreover, spearcons showed comparable learnability as normal speech and led to better performance than speech-only auditory cues in two-dimensional menu navigation. CONCLUSION: These results show that spearcons can be more effective than previous auditory cues in menu-based interfaces. APPLICATION: Spearcons have broadened the taxonomy of nonspeech auditory cues. Users can benefit from the application of spearcons in real devices.},
  langid = {english},
  keywords = {Acoustic Stimulation,Adolescent,Analysis of Variance,Auditory Perception,Cell Phone,{Computers, Handheld},Cues,Data Display,Female,Humans,Male,Sound,Speech,User-Computer Interface,Young Adult}
}

@article{wassermanExtendingStateTransition1985,
  title = {Extending {{State Transition Diagrams}} for the {{Specification}} of {{Human}}–{{Computer Interaction}}},
  author = {Wasserman, A.I.},
  date = {1985-08},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {SE-11},
  number = {8},
  pages = {699--713},
  issn = {1939-3520},
  doi = {10.1109/TSE.1985.232519},
  abstract = {User Software Engineering is a methodology for the specification and implementation of interactive information systems. An early step in the methodology is the creation of a formal executable description of the user interaction with the system, based on augmented state transition diagrams. This paper shows the derivation of the USE transition diagrams based on perceived shortcomings of the "pure" state transition diagram approach. In this way, the features of the USE specification notation are gradually presented and illustrated. The paper shows both the graphical notation and the textual equivalent of the notation, and briefly describes the automated tools that support direct execution of the specification.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {Command languages,Database languages,Design methodology,Executable specifications,Information science,Information systems,interactive information systems,Interactive systems,Programming,rapid prototyping,software development methodology,Software engineering,Software prototyping,transition diagrams,user interfaces,User interfaces,User Software Engineering},
  file = {C:\Users\jseo1005\Zotero\storage\HXFH2CCB\Wasserman - 1985 - Extending State Transition Diagrams for the Specif.pdf}
}

@book{wickhamDataScienceImport2016,
  title = {R for {{Data Science}}: {{Import}}, {{Tidy}}, {{Transform}}, {{Visualize}}, and {{Model Data}}},
  shorttitle = {R for {{Data Science}}},
  author = {Wickham, Hadley and Grolemund, Garrett},
  date = {2016-12-12},
  eprint = {I6y3DQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{"O'Reilly Media, Inc."}},
  abstract = {Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible.Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. You'll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what you've learned along the way.You'll learn how to:Wrangle—transform your datasets into a form convenient for analysisProgram—learn powerful R tools for solving data problems with greater clarity and easeExplore—examine your data, generate hypotheses, and quickly test themModel—provide a low-dimensional summary that captures true "signals" in your datasetCommunicate—learn R Markdown for integrating prose, code, and results},
  isbn = {978-1-4919-1034-4},
  langid = {english},
  pagetotal = {474},
  keywords = {Computers / Data Science / General,Computers / Mathematical \& Statistical Software,Mathematics / Probability \& Statistics / General}
}

@book{wickhamGgplot2ElegantGraphics2010,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  shorttitle = {Ggplot2},
  author = {Wickham, Hadley},
  date = {2010-02-22},
  edition = {1st ed. 2009. Corr. 3rd printing 2010 edition},
  publisher = {{Springer}},
  location = {{New York}},
  abstract = {Provides both rich theory and powerful applicationsFigures are accompanied by code required to produce themFull color figures},
  isbn = {978-0-387-98140-6},
  langid = {english},
  pagetotal = {213}
}

@article{wickhamLayeredGrammarGraphics2010,
  title = {A {{Layered Grammar}} of {{Graphics}}},
  author = {Wickham, Hadley},
  date = {2010-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  volume = {19},
  number = {1},
  pages = {3--28},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/jcgs.2009.07098},
  url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2009.07098},
  urldate = {2022-09-21},
  langid = {english},
  file = {C:\Users\jseo1005\Zotero\storage\7MXU8HHI\Wickham - 2010 - A Layered Grammar of Graphics.pdf}
}

@incollection{wilkinsonGrammarGraphics2012,
  title = {The {{Grammar}} of {{Graphics}}},
  booktitle = {Handbook of {{Computational Statistics}}: {{Concepts}} and {{Methods}}},
  author = {Wilkinson, Leland},
  editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
  date = {2012},
  series = {Springer {{Handbooks}} of {{Computational Statistics}}},
  pages = {375--414},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-21551-3_13},
  url = {https://doi.org/10.1007/978-3-642-21551-3_13},
  urldate = {2023-01-24},
  abstract = {The Grammar of Graphics, or GOG, denotes a system with seven orthogonal components. By orthogonal, we mean there are seven graphical component sets whose elements are aspects of the general system and that every combination of aspects in the product of all these sets is meaningful. This sense of the word orthogonality, a term used by computer designers to describe a combinatoric system of components or building blocks, is in some sense similar to the orthogonal factorial analysis of variance (ANOVA), where factors have levels and all possible combinations of levels exist in the ANOVA design. If we interpret each combination of features in a GOG system as a point in a network, then the world described by GOG is represented in a seven-dimensional rectangular lattice.},
  isbn = {978-3-642-21551-3},
  langid = {english},
  keywords = {Algebraic Expression,Graph Function,Graphic System,Recursive Partitioning,Statistical Graphic},
  file = {C:\Users\jseo1005\Zotero\storage\YCA8836K\Wilkinson - 2012 - The Grammar of Graphics.pdf}
}

@inproceedings{wuDataDataEverywhere2023,
  title = {Data, {{Data}}, {{Everywhere}}: {{Uncovering Everyday Data Experiences}} for {{People}} with {{Intellectual}} and {{Developmental Disabilities}}},
  shorttitle = {Data, {{Data}}, {{Everywhere}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wu, Keke and Tran, Michelle Ho and Petersen, Emma and Koushik, Varsha and Szafir, Danielle Albers},
  date = {2023-04-19},
  series = {{{CHI}} '23},
  pages = {1--17},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581204},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581204},
  urldate = {2023-04-25},
  abstract = {Data is everywhere but may not be accessible to everyone. Conventional data visualization tools and guidelines often do not actively consider the specific needs and abilities of people with Intellectual and Developmental Disabilities (IDD), leaving them excluded from data-driven activities and vulnerable to ethical issues. To understand the needs and challenges people with IDD have with data, we conducted 15 semi-structured interviews with individuals with IDD and their caregivers. Our algorithmic interview approach situated data in the lived experiences of people with IDD to uncover otherwise hidden data encounters in their everyday life. Drawing on findings and observations, we characterize how they conceptualize data, when and where they use data, and what barriers exist when they interact with data. We use our results as a lens to reimagine the role of visualization in data accessibility and establish a critical near-term research agenda for cognitively accessible visualization.},
  isbn = {978-1-4503-9421-5},
  keywords = {human-subjects qualitative studies,personal visual analytics},
  file = {C:\Users\jseo1005\Zotero\storage\HUD5FMNM\Wu et al. - 2023 - Data, Data, Everywhere Uncovering Everyday Data E.pdf}
}

@inproceedings{yangTactilePresentationNetwork2020,
  title = {Tactile {{Presentation}} of {{Network Data}}: {{Text}}, {{Matrix}} or {{Diagram}}?},
  shorttitle = {Tactile {{Presentation}} of {{Network Data}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Yang, Yalong and Marriott, Kim and Butler, Matthew and Goncu, Cagatay and Holloway, Leona},
  date = {2020-04-23},
  series = {{{CHI}} '20},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376367},
  url = {https://doi.org/10.1145/3313831.3376367},
  urldate = {2022-11-06},
  abstract = {Visualisations are commonly used to understand social, biological and other kinds of networks. Currently we do not know how to effectively present network data to people who are blind or have low-vision (BLV). We ran a controlled study with 8 BLV participants comparing four tactile representations: organic node-link diagram, grid node-link diagram, adjacency matrix and braille list. We found that the node-link representations were preferred and more effective for path following and cluster identification while the matrix and list were better for adjacency tasks. This is broadly in line with findings for the corresponding visual representations.},
  isbn = {978-1-4503-6708-0},
  keywords = {accessibility,adjacency matrix,blindness,graphvisualization,vision impairment},
  file = {C:\Users\jseo1005\Zotero\storage\58WSGZTQ\Yang et al. - 2020 - Tactile Presentation of Network Data Text, Matrix.pdf}
}

@article{zanellaSonificationSoundDesign2022,
  title = {Sonification and Sound Design for Astronomy Research, Education and Public Engagement},
  author = {Zanella, A. and Harrison, C. M. and Lenzi, S. and Cooke, J. and Damsma, P. and Fleming, S. W.},
  date = {2022-11},
  journaltitle = {Nature Astronomy},
  shortjournal = {Nat Astron},
  volume = {6},
  number = {11},
  pages = {1241--1248},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3366},
  doi = {10.1038/s41550-022-01721-z},
  url = {https://www.nature.com/articles/s41550-022-01721-z},
  urldate = {2023-03-11},
  abstract = {Over the past ten years there has been a large increase in the number of projects using sound to represent astronomical data and concepts. Motivation for these projects includes the potential to enhance scientific discovery within complex datasets, by utilizing the inherent multidimensionality of sound and the ability of our hearing to filter signals from noise. Other motivations include creating engaging multisensory resources, for education and public engagement, and making astronomy more accessible to people who are blind or have low vision, promoting their participation in science and related careers. We describe potential benefits of sound within these contexts and provide an overview of the nearly 100 sound-based astronomy projects that we have identified. We discuss current limitations and challenges of the approaches taken. Finally, we suggest future directions to help realize the full potential of sound-based techniques in general and to widen their application within the astronomy community.},
  issue = {11},
  langid = {english},
  keywords = {Astronomy and astrophysics,Interdisciplinary studies},
  file = {C:\Users\jseo1005\Zotero\storage\S5RZB7RG\Zanella et al. - 2022 - Sonification and sound design for astronomy resear.pdf}
}
